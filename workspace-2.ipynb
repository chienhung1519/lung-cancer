{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Structure\n",
    "\n",
    "* level 1: string number\n",
    "* level 2: context, NER_target\n",
    "* level 3: \n",
    "  * H_type: Histologic Type\n",
    "  * H_grade: Histologic Grade\n",
    "  * TF: Tumor Focality\n",
    "  * LV: Lymph-Vascular Invasion\n",
    "  * CM: closest margin\n",
    "  * size: size\n",
    "* level 4: \n",
    "  * content: extracted paragraph, if no will be `without content`\n",
    "  * annotate: target, if no will be `-`\n",
    "  * content_tag and NER_taging: no use currently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料筆數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "860"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context 為空的筆數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for idx, example in data.items() if example\"context\" == \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Length 分佈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1783"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", n_positions=2048)\n",
    "context = example\"context\" for idx, example in data.items()\n",
    "context_len = len(tokenizer(ctx)\"input_ids\") for ctx in context\n",
    "\n",
    "max(context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "資料介紹\n",
    "* NER_H&F_keycontent.json(./data/raw/NER_H%26F_keycontent.json): 人工驗證，共 207 名個案，860 則病理報告\n",
    "* NER_keycontent_708patients.json(./data/raw/NER_keycontent_708patients.json): 規則抓取，共 501 名個案，2187 則病理報告 + NER_H&F_keycontent.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER target\n",
    "* **soap report** / **cetology report**: organ, Bx-site, sampling method, diagosis\n",
    "* **path report**: H_type, H_grade, TF, LV, CM, size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "觀察 NER_keycontent_708patients.json(./data/raw/NER_keycontent_708patients.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soap report max lenhth: 962\n",
      "path report max length: 2160\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data = json.loads(Path(\"./data/raw/NER_keycontent_708patients.json\").read_text())\n",
    "Path(\"./data/raw/NER_H&F_keycontent.json\").write_text(json.dumps(data, indent=2))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", n_positions=2048)\n",
    "soap_report = example\"soap report\" for idx, example in data.items()\n",
    "path_report = example\"path report\" for idx, example in data.items()\n",
    "soap_len = len(tokenizer(report)\"input_ids\") for report in soap_report\n",
    "path_len = len(tokenizer(report)\"input_ids\") for report in path_report\n",
    "\n",
    "print(f\"soap report max lenhth: {max(soap_len)}\\npath report max length: {max(path_len)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 874\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "data = json.loads(Path(\"./data/raw/NER_keycontent_708patients.json\").read_text())\n",
    "Path(\"./data/raw/NER_H&F_keycontent.json\").write_text(json.dumps(data, indent=2))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\", n_positions=2048)\n",
    "\n",
    "content = items\"content\" for idx, example in data.items() for target, items in example\"NER_target\".items()\n",
    "content = list(set(content))\n",
    "content_len = len(tokenizer(cont)\"input_ids\") for cont in content\n",
    "\n",
    "print(f\"max length: {max(content_len)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有哪些中文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['主治醫師', '余代和', '余鐵國', '健保署', '傅安生', '切片日期', '劉伯義', '劉平', '劉慶綬', '劉文賢', '劉方來', '劉榮木', '劉瑞雄', '劉瑞雲', '劉興基', '加註類別', '博仁醫院', '原位癌', '原本', '參考文獻', '口病專醫字第', '台北病理中心', '台安醫院', '史莊廷妹', '名字', '吳世雄', '吳文賢', '吳朝褔', '吳錦章', '吳錫全', '周中明', '周坤輝', '周彩霞', '周文章', '周朝榮', '周泰源', '周義雄', '周邱登美', '喬美華', '嚴敏禎', '報告日期', '壢新醫院', '夏德興', '姚菊瑛', '孟慶德', '孫德寶', '孫殿文', '孫靖嫺', '孫鳳舉', '宋騰琰', '年新出版', '廖', '廖林月花', '廖笑', '廖純霞', '張世欣', '張嬌蓮', '張宜崴', '張景晃', '張林秀賢', '張水源', '張水返', '張清根', '張秋煌', '張維富', '張鳴', '張齡材', '徐文芳', '徐辰芳', '忠孝醫院', '戴光明', '承王延秀', '振興醫療財團法人振興醫院', '振興醫院', '收件日期', '方台生', '方嘉郎', '方洪昌', '施隆彬', '景薇立', '更改加註類別', '更正為', '曾修良', '曾塗盛', '曾正均', '曾玉龍', '曾良玉', '曾蔡郁姬', '朱娟秀', '朱家瑩', '朱明細', '朱錫卿', '李', '李寶純', '李德發', '李振揚', '李方玉翠', '李明義', '李素真', '李詩鐘', '杜士英', '杜美瑤', '林余香', '林周清香', '林幸蓁', '林廖炎珠', '林志昌', '林忠華', '林文雄', '林智化', '林月波', '林月秀', '林柑', '林武龍', '林永和', '林芳美', '林蘇坤', '林賜恩', '林進祥', '林達雄', '林金在', '林金城', '林錦旺', '林麗平', '林麗淑', '林黃淑櫻', '柳天送', '梁丁海光', '楊勝雄', '楊志能', '楊朝全', '楊正治', '楊秋素', '楊鴻育', '檢字第', '檢體', '檢體編號', '正確的編碼為', '江榮富', '汪清雲', '沈長發', '洪文權', '洪水鄭', '溫玉輝', '潘瓊珠', '潘練保鳳', '潘雲章', '王一男', '王世民', '王京子', '王偉民', '王先本', '王嘯龍', '王坤章', '王張秀蘭', '王永毅', '王登濬', '王蔡幸', '理號碼', '璩懷水', '甯子慧', '由', '病理號', '病理號碼', '病理診斷', '病解專醫字第', '石秀娟', '祝慧珍', '組織報告', '結果', '經由台灣病理學會向', '編碼為', '羅周秀琴', '翁婉淨', '翁林澄', '胡健民', '胡淑華', '腫瘤細胞不足', '至少原位癌', '臺北市立聯合醫院忠孝院區', '臺北病理中心', '臺安醫院', '范均', '莊輝雄', '葉信和', '葉峻廷', '葉德旺', '葉斯德', '葉洪賢', '葉玉枝', '蔡佳儒', '蔡凱玲', '蔡敦厚', '蔡滄海', '蔡漢卿', '蔡珠員', '蔡進忠', '蔡阿妙', '蔡陳蕉蓉', '蕭盧秀英', '蕭秀梅', '藍家和', '藍海泉', '藍雪卿', '蘇倩儀', '蘇南山', '蘇永忠', '蘇秋梅', '號', '許永爾', '詢問後確認紙本為誤植', '詹益棟', '謝兆棟', '謝吳明月', '謝彩橋', '謝添福', '謝秋英', '賴崇慶', '賴陳清香', '趙曉梅', '趙田裕蘭', '退件', '邱于芳', '邱健', '邱振興', '邱榮江', '邱秀珠', '邱順舟', '部立臺北醫院', '郭慧敏', '郭木欽', '郭秀玉', '郭秋花', '郭艷華', '鄒培芳', '鄭建', '鄭建睿', '鄭德', '鄭泳松', '鄭義隆', '鍾懿莉', '阮榮錦', '阮照雄', '陳來進', '陳劉碧鳳', '陳勝源', '陳吳金蕊', '陳威宇', '陳宏棋', '陳志榮', '陳明裕', '陳朝順', '陳榕偵', '陳榮通', '陳樹凱', '陳淑戀', '陳淑美', '陳淑麗', '陳炳坤', '陳玉', '陳玉琴', '陳睿穎', '陳素月', '陳鄭玉姬', '陳金塗', '陳雲英', '陳韋瑞', '陳飛宗', '雙和醫院', '顏明達', '顏瑞彤', '顧文輝', '顧潔光', '高文華', '高正和', '高瑞峨', '高肇顯', '高華璘', '魏鴻基', '黃哲文', '黃坤雄', '黃子芳', '黃宗雄', '黃容華', '黃文彰', '黃春櫻', '黃清貴', '黃瑞文', '黃秋榮', '黃簡秀滿', '黃耀輝', '黃興國', '黃長發', '黃雪梅', '黃麗英', '黎愛珠', '齊鶯卿']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = json.loads(Path(\"./data/raw/NER_keycontent_708patients.json\").read_text())\n",
    "Path(\"./data/raw/NER_H&F_keycontent.json\").write_text(json.dumps(data, indent=2))\n",
    "\n",
    "words = \n",
    "zhPattern = re.compile(u'\\u4e00-\\u9fa5+')\n",
    "for idx, example in data.items():\n",
    "    for report in \"soap report\", \"path report\", \"cetology report\":\n",
    "        match = zhPattern.findall(examplereport)\n",
    "        words.extend(match)\n",
    "\n",
    "print(sorted(list(set(words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除中文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data = json.loads(Path(\"./data/raw/NER_keycontent_708patients.json\").read_text())\n",
    "Path(\"./data/raw/NER_H&F_keycontent.json\").write_text(json.dumps(data, indent=2))\n",
    "\n",
    "zhPattern = re.compile(u'\\u4e00-\\u9fa5+')\n",
    "for idx, example in data.items():\n",
    "    for report in \"soap report\", \"path report\", \"cetology report\":\n",
    "        match = zhPattern.findall(examplereport)\n",
    "        for word in match:\n",
    "            examplereport = examplereport.replace(word, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分資料集\n",
    "* 在假設病理報告間是彼此獨立的前提下切分資料\n",
    "* 不過觀察資料的過程中，感覺前後順序有點關係"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1809967"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = json.loads(Path(\"./data/raw/NER_keycontent_708patients.json\").read_text())\n",
    "Path(\"./data/raw/NER_H&F_keycontent.json\").write_text(json.dumps(data, indent=2))\n",
    "\n",
    "# Remove Chinese\n",
    "zhPattern = re.compile(u'\\u4e00-\\u9fa5+')\n",
    "for idx, example in data.items():\n",
    "    for target, items in example\"NER_target\".items():\n",
    "        match = zhPattern.findall(items\"content\")\n",
    "        for word in match:\n",
    "            items\"content\" = items\"content\".replace(word, \"\")\n",
    "\n",
    "seed = 1209\n",
    "train_ratio = 0.8\n",
    "data_ids = np.array(int(idx) for idx in data.keys())\n",
    "train_ids, test_ids = train_test_split(data_ids, shuffle=True, random_state=seed, train_size=train_ratio)\n",
    "train = datastr(idx) for idx in train_ids\n",
    "test = datastr(idx) for idx in test_ids\n",
    "Path(\"./data/processed/train.json\").write_text(json.dumps(train, indent=2, ensure_ascii=False))\n",
    "Path(\"./data/processed/test.json\").write_text(json.dumps(test, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "soap_target = \"organ\", \"Bx-site\", \"sampling method\", \"diagnosis\"\n",
    "path_target = \"H_type\", \"H_grade\", \"TF\", \"LV\", \"CM\", \"size\"\n",
    "\n",
    "target_to_question = {\n",
    "    \"organ\": \"What is the organ?\", \n",
    "    \"Bx-site\": \"What is the Bx-site?\", \n",
    "    \"sampling method\": \"What is the sampling method?\", \n",
    "    \"diagnosis\": \"What is the diagnosis?\",\n",
    "    \"H_type\": \"What is the value of Histologic Type?\", \n",
    "    \"H_grade\": \"What is the value of Histologic Grade?\", \n",
    "    \"TF\": \"What is the value of Tumor Focality?\", \n",
    "    \"LV\": \"What is the value of Lymph-Vascular Invasion?\", \n",
    "    \"CM\": \"What is the value of closest margin?\", \n",
    "    \"size\": \"What is the value of tumor size?\"\n",
    "}\n",
    "\n",
    "data_path = {\n",
    "    \"train\": \"./data/processed/train.json\",\n",
    "    \"test\": \"./data/processed/test.json\"\n",
    "}\n",
    "\n",
    "input_data = defaultdict(list)\n",
    "for split, path in data_path.items():\n",
    "    split_data = json.loads(Path(path).read_text())\n",
    "    for example in split_data:\n",
    "        for target in soap_target:\n",
    "            if example\"soap report\" == \"\":\n",
    "                assert example'cetology report' != \"\"\n",
    "                input_datasplit.append(\n",
    "                    \"question\",\n",
    "                    f\"{target_to_questiontarget} context: {example'cetology report'} NO\",\n",
    "                    \"NO\" if example\"NER_target\"target\"annotate\" == \"-\" else example\"NER_target\"target\"annotate\"\n",
    "                )\n",
    "            else:\n",
    "                if example'soap report' == \"\":\n",
    "                    continue\n",
    "                input_datasplit.append(\n",
    "                    \"question\",\n",
    "                    f\"{target_to_questiontarget} context: {example'soap report'} NO\",\n",
    "                    \"NO\" if example\"NER_target\"target\"annotate\" == \"-\" else example\"NER_target\"target\"annotate\"\n",
    "                )\n",
    "        for target in path_target:\n",
    "            if example'path report' == \"\":\n",
    "                continue\n",
    "            input_datasplit.append(\n",
    "                \"question\",\n",
    "                f\"{target_to_questiontarget} context: {example'path report'} NO\",\n",
    "                \"NO\" if example\"NER_target\"target\"annotate\" == \"-\" else example\"NER_target\"target\"annotate\"\n",
    "            )\n",
    "\n",
    "train_df = pd.DataFrame(input_data\"train\")\n",
    "train_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "train_df.to_excel(\"./data/processed/train.xlsx\", index=False)\n",
    "\n",
    "test_df = pd.DataFrame(input_data\"test\")\n",
    "test_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "SKIP_SIGN = \"without content\"\n",
    "NO_TARGET = \"-\"\n",
    "NO_TEXT = \"NO\"\n",
    "\n",
    "target_to_question = {\n",
    "    \"organ\": \"What is the organ?\", \n",
    "    \"Bx-site\": \"What is the Bx-site?\", \n",
    "    \"sampling method\": \"What is the sampling method?\", \n",
    "    \"diagnosis\": \"What is the diagnosis?\",\n",
    "    \"H_type\": \"What is the value of Histologic Type?\", \n",
    "    \"H_grade\": \"What is the value of Histologic Grade?\", \n",
    "    \"TF\": \"What is the value of Tumor Focality?\", \n",
    "    \"LV\": \"What is the value of Lymph-Vascular Invasion?\", \n",
    "    \"CM\": \"What is the value of closest margin?\", \n",
    "    \"size\": \"What is the value of tumor size?\"\n",
    "}\n",
    "\n",
    "target_to_prefix = {\n",
    "    \"organ\": \"question\", \n",
    "    \"Bx-site\": \"biopsy\", \n",
    "    \"sampling method\": \"question\", \n",
    "    \"diagnosis\": \"diagnosis\",\n",
    "    \"H_type\": \"question\", \n",
    "    \"H_grade\": \"question\", \n",
    "    \"TF\": \"question\", \n",
    "    \"LV\": \"question\", \n",
    "    \"CM\": \"question\", \n",
    "    \"size\": \"question\"\n",
    "}\n",
    "\n",
    "data_path = {\n",
    "    \"train\": \"./data/processed/train.json\",\n",
    "    \"test\": \"./data/processed/test.json\"\n",
    "}\n",
    "\n",
    "input_data = defaultdict(list)\n",
    "for split, path in data_path.items():\n",
    "    split_data = json.loads(Path(path).read_text())\n",
    "    for example in split_data:\n",
    "        for target, items in example\"NER_target\".items():\n",
    "            if items\"content\" == SKIP_SIGN:\n",
    "                continue\n",
    "            \n",
    "            prefix = target_to_prefixtarget\n",
    "            input_text = f\"{target_to_questiontarget} context: {items'content'} {NO_TEXT}\"\n",
    "            input_text = re.sub(\"\\s+\", \" \", input_text)\n",
    "            target_text = NO_TEXT if items\"annotate\" == NO_TARGET else items\"annotate\"\n",
    "            input_datasplit.append(prefix, input_text, target_text)\n",
    "\n",
    "train_df = pd.DataFrame(input_data\"train\")\n",
    "train_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "train_df.to_excel(\"./data/processed/train.xlsx\", index=False)\n",
    "\n",
    "test_df = pd.DataFrame(input_data\"test\")\n",
    "test_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['question: What is the organ? context: <MALIGNANCY> Lung, right lower lobe, CT-guided needle biopsy, small cell carcinoma NULL',\n",
       " 'biopsy: What is the Bx-site? context: <MALIGNANCY> Lung, right lower lobe, CT-guided needle biopsy, small cell carcinoma NULL',\n",
       " 'question: What is the sampling method? context: <MALIGNANCY> Lung, right lower lobe, CT-guided needle biopsy, small cell carcinoma NULL']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "to_predict:3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the organ? context: Intestine, large, ...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biopsy</td>\n",
       "      <td>What is the Bx-site? context: Intestine, large...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the sampling method? context: Intestin...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diagnosis</td>\n",
       "      <td>What is the diagnosis? context: Intestine, lar...</td>\n",
       "      <td>NULL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the organ? context: Lung, main bronchu...</td>\n",
       "      <td>Lung</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prefix                                         input_text target_text\n",
       "0   question  What is the organ? context: Intestine, large, ...        NULL\n",
       "1     biopsy  What is the Bx-site? context: Intestine, large...        NULL\n",
       "2   question  What is the sampling method? context: Intestin...        NULL\n",
       "3  diagnosis  What is the diagnosis? context: Intestine, lar...        NULL\n",
       "4   question  What is the organ? context: Lung, main bronchu...        Lung"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prefix</th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the organ? context: &lt;MALIGNANCY&gt; Lung,...</td>\n",
       "      <td>Lung</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>biopsy</td>\n",
       "      <td>What is the Bx-site? context: &lt;MALIGNANCY&gt; Lun...</td>\n",
       "      <td>RLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the sampling method? context: &lt;MALIGNA...</td>\n",
       "      <td>CT-guided needle biopsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>diagnosis</td>\n",
       "      <td>What is the diagnosis? context: &lt;MALIGNANCY&gt; L...</td>\n",
       "      <td>carcinoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>question</td>\n",
       "      <td>What is the organ? context: Pathologic Report ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prefix                                         input_text  \\\n",
       "0   question  What is the organ? context: <MALIGNANCY> Lung,...   \n",
       "1     biopsy  What is the Bx-site? context: <MALIGNANCY> Lun...   \n",
       "2   question  What is the sampling method? context: <MALIGNA...   \n",
       "3  diagnosis  What is the diagnosis? context: <MALIGNANCY> L...   \n",
       "4   question  What is the organ? context: Pathologic Report ...   \n",
       "\n",
       "               target_text  \n",
       "0                     Lung  \n",
       "1                      RLL  \n",
       "2  CT-guided needle biopsy  \n",
       "3                carcinoma  \n",
       "4                      NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845a7c6fcf0c485e84d6bfdcc28bf2e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3478: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8595c3acd8664c0fa1520bf6d0181918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740e4a798db14b26ab14bc9ea28fa4d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 5:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2082088e7144851ac957665e92ab3c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 5:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1f878ec5bd4f958a913c674be4ec9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 5:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffd5d07d15346abb98f231f9fcffdbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 5:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4fa8f64de04265b8dfd6a7de224bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 5:   0%|          | 0/1394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604d383627b648458ffe71756817b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/358 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1a76301cf34452808b06f9486e95cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/2862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference: https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b\n",
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "train_df = pd.read_excel(\"./data/processed/train.xlsx\", dtype=str)\n",
    "test_df = pd.read_excel(\"./data/processed/test.xlsx\", dtype=str)\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "\n",
    "# Configure the model\n",
    "# General args: https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model\n",
    "# T5 args: https://simpletransformers.ai/docs/t5-model/\n",
    "# If memory problem occurs, set lower max_seq_length or train_batch_size\n",
    "model_args = T5Args()\n",
    "model_args.manual_seed = 1209\n",
    "model_args.max_seq_length = 900\n",
    "model_args.train_batch_size = 8\n",
    "model_args.num_train_epochs = 5\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.n_gpu = 2\n",
    "\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, use_cuda=True)\n",
    "\n",
    "# Make predictions with the model\n",
    "preds = model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "test_df\"prediction\" = preds\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 88.02 %; number is 2862\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "labels = test_df.target_text.values.tolist()\n",
    "preds = test_df.prediction.values.tolist()\n",
    "\n",
    "acc = sum(1 for l, p in zip(labels, preds) if l == p) / len(labels)\n",
    "print(f\"Accuracy is {round(acc, 4) * 100} %; number is {len(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organ number is 2437\n",
      "Bx-site number is 2437\n",
      "sampling method number is 2437\n",
      "diagnosis number is 2437\n",
      "H_type number is 210\n",
      "H_grade number is 257\n",
      "TF number is 204\n",
      "LV number is 210\n",
      "CM number is 180\n",
      "size number is 342\n",
      "size total number is 11151\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_excel_path = \"./data/processed/train.xlsx\"\n",
    "train_df = pd.read_excel(train_excel_path)\n",
    "\n",
    "target_to_question = {\n",
    "    \"organ\": \"What is the organ?\", \n",
    "    \"Bx-site\": \"What is the Bx-site?\", \n",
    "    \"sampling method\": \"What is the sampling method?\", \n",
    "    \"diagnosis\": \"What is the diagnosis?\",\n",
    "    \"H_type\": \"What is the value of Histologic Type?\", \n",
    "    \"H_grade\": \"What is the value of Histologic Grade?\", \n",
    "    \"TF\": \"What is the value of Tumor Focality?\", \n",
    "    \"LV\": \"What is the value of Lymph-Vascular Invasion?\", \n",
    "    \"CM\": \"What is the value of closest margin?\", \n",
    "    \"size\": \"What is the value of tumor size?\"\n",
    "}\n",
    "\n",
    "for target, question in target_to_question.items():\n",
    "    t_labels = row.target_text for row in train_df.itertuples() if question in row.input_text\n",
    "    print(f\"{target} number is {len(t_labels)}\")\n",
    "\n",
    "print(f\"Total number is {len(train_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organ accuracy is 96.89 %; number is 610\n",
      "Bx-site accuracy is 83.28 %; number is 610\n",
      "sampling method accuracy is 95.41 %; number is 610\n",
      "diagnosis accuracy is 85.57000000000001 %; number is 610\n",
      "H_type accuracy is 40.32 %; number is 62\n",
      "H_grade accuracy is 93.33 %; number is 75\n",
      "TF accuracy is 87.1 %; number is 62\n",
      "LV accuracy is 87.88 %; number is 66\n",
      "CM accuracy is 85.96000000000001 %; number is 57\n",
      "size accuracy is 60.0 %; number is 100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "target_to_question = {\n",
    "    \"organ\": \"What is the organ?\", \n",
    "    \"Bx-site\": \"What is the Bx-site?\", \n",
    "    \"sampling method\": \"What is the sampling method?\", \n",
    "    \"diagnosis\": \"What is the diagnosis?\",\n",
    "    \"H_type\": \"What is the value of Histologic Type?\", \n",
    "    \"H_grade\": \"What is the value of Histologic Grade?\", \n",
    "    \"TF\": \"What is the value of Tumor Focality?\", \n",
    "    \"LV\": \"What is the value of Lymph-Vascular Invasion?\", \n",
    "    \"CM\": \"What is the value of closest margin?\", \n",
    "    \"size\": \"What is the value of tumor size?\"\n",
    "}\n",
    "\n",
    "for target, question in target_to_question.items():\n",
    "    t_labels = row.target_text for row in test_df.itertuples() if question in row.input_text\n",
    "    t_preds = row.prediction for row in test_df.itertuples() if question in row.input_text\n",
    "    acc = sum(1 for l, p in zip(t_labels, t_preds) if l == p) / len(t_labels)\n",
    "    print(f\"{target} accuracy is {round(acc, 4) * 100} %; number is {len(t_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "target_to_question = {\n",
    "    \"organ\": \"What is the organ?\", \n",
    "    \"Bx-site\": \"What is the Bx-site?\", \n",
    "    \"sampling method\": \"What is the sampling method?\", \n",
    "    \"diagnosis\": \"What is the diagnosis?\",\n",
    "    \"H_type\": \"What is the value of Histologic Type?\", \n",
    "    \"H_grade\": \"What is the value of Histologic Grade?\", \n",
    "    \"TF\": \"What is the value of Tumor Focality?\", \n",
    "    \"LV\": \"What is the value of Lymph-Vascular Invasion?\", \n",
    "    \"CM\": \"What is the value of closest margin?\", \n",
    "    \"size\": \"What is the value of tumor size?\"\n",
    "}\n",
    "\n",
    "output = {\"target\": , \"text\": , \"label\": , \"prediction\": }\n",
    "for target, question in target_to_question.items():\n",
    "    for row in test_df.itertuples():\n",
    "        if question in row.input_text:\n",
    "            text = row.input_text\n",
    "            text = text.replace(f\"{question} context: \", \"\")\n",
    "            text = text:-3\n",
    "            if row.target_text != row.prediction:\n",
    "                output\"target\".append(target)\n",
    "                output\"text\".append(text)\n",
    "                output\"label\".append(\"NULL\" if row.target_text == \"NO\" else row.target_text)\n",
    "                output\"prediction\".append(\"NULL\" if row.prediction == \"NO\" else row.prediction)\n",
    "output = pd.DataFrame(output)\n",
    "output.to_excel(\"./outputs/errors.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加入分類任務資料一起訓練\n",
    "* 只有 `organ`, `box-site`, `sampling method`, `diagnosis` 是看 **cetology report** 和 **soap report**\n",
    "* 其他的都是 **pathology report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每個 target 的類別數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59761"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/raw/merge_keycontent.json\"\n",
    "data = json.loads(Path(data_path).read_text())\n",
    "\n",
    "targets =  list(data\"1\"\"NER_target\".keys())\n",
    "target_cat = {}\n",
    "for target in targets:\n",
    "    cat = \n",
    "    for idx, example in data.items():\n",
    "        annotate = example\"NER_target\"target\"annotate\"\n",
    "        if annotate != \"-\":\n",
    "            cat.append(str(annotate).strip())\n",
    "    target_cattarget = {\"count\": len(cat), \"category\": {c: cat.count(c) for c in set(cat)}}\n",
    "Path(\"./save/target_cat.json\").write_text(json.dumps(target_cat, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算每個 target 的標註出現在原文的覆蓋率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/raw/merge_keycontent.json\"\n",
    "data = json.loads(Path(data_path).read_text())\n",
    "\n",
    "targets =  list(data\"1\"\"NER_target\".keys())\n",
    "target_coverage = {}\n",
    "for target in targets:\n",
    "    coverage = 0\n",
    "    total = 0\n",
    "    for idx, example in data.items():\n",
    "        annotate = str(example\"NER_target\"target\"annotate\")\n",
    "        content = str(example\"NER_target\"target\"content\")\n",
    "        if annotate != \"-\":\n",
    "            total += 1\n",
    "            if annotate in example\"soap report\":\n",
    "                coverage += 1\n",
    "            elif annotate in example\"path report\":\n",
    "                coverage += 1\n",
    "            elif annotate in example\"cetology report\":\n",
    "                coverage += 1\n",
    "    target_coveragetarget = f\"{round(coverage / total, 4) * 100} %\"\n",
    "Path(\"./save/target_coverage.json\").write_text(json.dumps(target_coverage, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "切分資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5560052"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = \"./data/raw/merge_keycontent.json\"\n",
    "data = json.loads(Path(data_path).read_text())\n",
    "\n",
    "seed = 1209\n",
    "train_ratio = 0.8\n",
    "data_ids = list(data.keys())\n",
    "train_ids, test_ids = train_test_split(data_ids, shuffle=True, random_state=seed, train_size=train_ratio)\n",
    "train = dataidx for idx in train_ids\n",
    "test = dataidx for idx in test_ids\n",
    "Path(\"./data/processed/train.json\").write_text(json.dumps(train, indent=2, ensure_ascii=False))\n",
    "Path(\"./data/processed/test.json\").write_text(json.dumps(test, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2192"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "target_to_question = {\n",
    "    'organ': \"What is the organ?\",\n",
    "    'Bx-site': \"What is the Bx-site?\",\n",
    "    'sampling method': \"What is the sampling method?\",\n",
    "    'diagnosis': \"What is the diagnosis?\",\n",
    "    'size': \"What is the value of tumor size?\",\n",
    "    'Greatest_dimension': \"What is the value of greatest dimension?\",\n",
    "    'H_type': \"What is the value of Histologic Type?\",\n",
    "    'H_grade': \"What is the value of Histologic Grade?\",\n",
    "    'TF': \"What is the value of Tumor Focality?\",\n",
    "    'LV': \"What is the value of Lymph-Vascular Invasion?\",\n",
    "    'CM': \"What is the value of closest margin?\",\n",
    "    'VPI': \"What is the value of VPI?\",\n",
    "    'EGFR': \"What is the EGFR?\",\n",
    "    'ALK': \"Is ALK Positive, Negative, or Unkown?\",\n",
    "    'ROS1': \"Is ROS1 Positive, Negative, or Unkown?\",\n",
    "    'BRAF': \"Is BRAF Positive, Negative, or Unkown?\",\n",
    "    'MET': \"Is MET Positive, Negative, or Unkown?\",\n",
    "    'KRAS': \"Is KRAS Positive, Negative, or Unkown?\",\n",
    "    'ERBB2': \"Is ERBB2 Positive, Negative, or Unkown?\",\n",
    "    'PIK3CA': \"Is PIK3CA Positive, Negative, or Unkown?\",\n",
    "    'NRAS': \"Is NRAS Positive, Negative, or Unkown?\",\n",
    "    'MEK1': \"Is MEK1 Positive, Negative, or Unkown?\",\n",
    "    'NTRK': \"Is NTRK Positive, Negative, or Unkown?\",\n",
    "    'RET': \"Is RET Positive, Negative, or Unkown?\",\n",
    "    'PDL1': \"What is the PDL1?\",\n",
    "    'ver': \"What is the version?\",\n",
    "    'pT': \"What is the pT?\",\n",
    "    'pN': \"What is the pN?\",\n",
    "    'pM': \"What is the pM?\",\n",
    "    'pStage': \"What is the pStage?\",\n",
    "    'CK7': \"Is CK7 Positive, Negative, or Unkown?\",\n",
    "    'TTF': \"Is TTF Positive, Negative, or Unkown?\",\n",
    "    'Napsin': \"Is Napsin Positive, Negative, or Unkown?\",\n",
    "    'CK20': \"Is CK20 Positive, Negative, or Unkown?\",\n",
    "    'P40': \"Is P40 Positive, Negative, or Unkown?\",\n",
    "    'CDX2': \"Is CDX2 Positive, Negative, or Unkown?\",\n",
    "    'P63': \"Is P63 Positive, Negative, or Unkown?\",\n",
    "    'P16': \"Is P16 Positive, Negative, or Unkown?\",\n",
    "    'cytokeratin': \"Is cytokeratin Positive, Negative, or Unkown?\",\n",
    "    'Vimentin': \"Is Vimentin Positive, Negative, or Unkown?\",\n",
    "    'PAX': \"Is PAX Positive, Negative, or Unkown?\",\n",
    "    'CD56': \"Is CD56 Positive, Negative, or Unkown?\",\n",
    "    'chromogranin': \"Is chromogranin Positive, Negative, or Unkown?\",\n",
    "    'synaptophysin': \"Is synaptophysin Positive, Negative, or Unkown?\",\n",
    "    'GATA3': \"Is GATA3 Positive, Negative, or Unkown?\"\n",
    "}\n",
    "Path(\"./save/target_to_question.json\").write_text(json.dumps(target_to_question, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "956"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "target_to_prefix = {\n",
    "    'organ': \"question\",\n",
    "    'Bx-site': \"biopsy\",\n",
    "    'sampling method': \"question\",\n",
    "    'diagnosis': \"diagnosis\",\n",
    "    'size': \"question\",\n",
    "    'Greatest_dimension': \"question\",\n",
    "    'H_type': \"question\",\n",
    "    'H_grade': \"question\",\n",
    "    'TF': \"question\",\n",
    "    'LV': \"question\",\n",
    "    'CM': \"question\",\n",
    "    'VPI': \"question\",\n",
    "    'EGFR': \"EGFR\",\n",
    "    'ALK': \"ALK\",\n",
    "    'ROS1': \"ROS1\",\n",
    "    'BRAF': \"BRAF\",\n",
    "    'MET': \"MET\",\n",
    "    'KRAS': \"KRAS\",\n",
    "    'ERBB2': \"ERBB2\",\n",
    "    'PIK3CA': \"PIK3CA\",\n",
    "    'NRAS': \"NRAS\",\n",
    "    'MEK1': \"MEK1\",\n",
    "    'NTRK': \"NTRK\",\n",
    "    'RET': \"RET\",\n",
    "    'PDL1': \"PDL1\",\n",
    "    'ver': \"question\",\n",
    "    'pT': \"question\",\n",
    "    'pN': \"question\",\n",
    "    'pM': \"question\",\n",
    "    'pStage': \"question\",\n",
    "    'CK7': \"CK7\",\n",
    "    'TTF': \"TTF\",\n",
    "    'Napsin': \"Napsin\",\n",
    "    'CK20': \"CK20\",\n",
    "    'P40': \"P40\",\n",
    "    'CDX2': \"CDX2\",\n",
    "    'P63': \"P63\",\n",
    "    'P16': \"P16\",\n",
    "    'cytokeratin': \"cytokeratin\",\n",
    "    'Vimentin': \"Vimentin\",\n",
    "    'PAX': \"PAX\",\n",
    "    'CD56': \"CD56\",\n",
    "    'chromogranin': \"chromogranin\",\n",
    "    'synaptophysin': \"synaptophysin\",\n",
    "    'GATA3': \"GATA3\"\n",
    "}\n",
    "Path(\"./save/target_to_prefix.json\").write_text(json.dumps(target_to_prefix, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "soap_targets = 'organ', 'Bx-site', 'sampling method', 'diagnosis'\n",
    "Path(\"./save/soap_targets.json\").write_text(json.dumps(soap_targets, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "NO_TARGET = \"-\"\n",
    "NO_TEXT = \"NO\"\n",
    "USE_CONTENT = False\n",
    "\n",
    "data_path = {\n",
    "    \"train\": \"./data/processed/train.json\",\n",
    "    \"test\": \"./data/processed/test.json\"\n",
    "}\n",
    "\n",
    "input_data = defaultdict(list)\n",
    "for split, path in data_path.items():\n",
    "    split_data = json.loads(Path(path).read_text())\n",
    "    for example in split_data:\n",
    "        for target, items in example\"NER_target\".items():\n",
    "            prefix = target_to_prefixtarget\n",
    "            question = target_to_questiontarget\n",
    "            if USE_CONTENT:\n",
    "                if target not in soap_targets and example\"path report\" == \"\":\n",
    "                    continue\n",
    "                input_text = f\"{question} context: {items'content'} {NO_TEXT}\"\n",
    "                input_text = re.sub(\"\\s+\", \" \", input_text)\n",
    "                target_text = NO_TEXT if items\"annotate\" == NO_TARGET else items\"annotate\"\n",
    "                input_datasplit.append(prefix, input_text, target_text)\n",
    "            else:\n",
    "                if target in soap_targets:\n",
    "                    if example\"soap report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'soap report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        input_text = f\"{question} context: {example'cetology report'} {NO_TEXT}\"\n",
    "                else:\n",
    "                    if example\"path report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'path report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        continue\n",
    "                input_text = re.sub(\"\\s+\", \" \", input_text)\n",
    "                target_text = NO_TEXT if items\"annotate\" == NO_TARGET else items\"annotate\"\n",
    "                input_datasplit.append(prefix, input_text, target_text)\n",
    "                    \n",
    "\n",
    "train_df = pd.DataFrame(input_data\"train\")\n",
    "train_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "train_df.to_excel(\"./data/processed/train.xlsx\", index=False)\n",
    "\n",
    "test_df = pd.DataFrame(input_data\"test\")\n",
    "test_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227196b28cac47c19d786923daaf36dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109665 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3478: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cb935fb1f645c58c5bd73ec47a90bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5636f745020f421292a0d278f065748b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 3:   0%|          | 0/27417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390cdd0a4c348d1bc6784f843a59323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 3:   0%|          | 0/27417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8af8b6b13ec4e11ad19fe5f6f1a58ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 3:   0%|          | 0/27417 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f1354d7c254abbbf5dbb473887ad3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3432 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4c24c69a924cdda11fb70f1792ff72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/27450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference: https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b\n",
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "train_df = pd.read_excel(\"./data/processed/train.xlsx\", dtype=str)\n",
    "test_df = pd.read_excel(\"./data/processed/test.xlsx\", dtype=str)\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "\n",
    "# Configure the model\n",
    "# General args: https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model\n",
    "# T5 args: https://simpletransformers.ai/docs/t5-model/\n",
    "# If memory problem occurs, set lower max_seq_length or train_batch_size\n",
    "model_args = T5Args()\n",
    "model_args.manual_seed = 1209\n",
    "model_args.max_seq_length = 900\n",
    "model_args.train_batch_size = 4\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.n_gpu = 2\n",
    "\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, use_cuda=True)\n",
    "\n",
    "# Make predictions with the model\n",
    "preds = model.predict(to_predict)\n",
    "test_df\"prediction\" = preds\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5784bcb01445989f97c17f74a3e26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating outputs:   0%|          | 0/3073 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3478: FutureWarning: \n",
      "`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n",
      "`__call__` method to prepare your inputs and the tokenizer under the `as_target_tokenizer` context manager to prepare\n",
      "your targets.\n",
      "\n",
      "Here is a short example:\n",
      "\n",
      "model_inputs = tokenizer(src_texts, ...)\n",
      "with tokenizer.as_target_tokenizer():\n",
      "    labels = tokenizer(tgt_texts, ...)\n",
      "model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
      "\n",
      "See the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\n",
      "For a more complete example, see the implementation of `prepare_seq2seq_batch`.\n",
      "\n",
      "  warnings.warn(formatted_warning, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fdbef12e1745fd8a88cef85278383c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decoding outputs:   0%|          | 0/24580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions with the model\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "preds = model.predict(to_predict)\n",
    "test_df\"prediction\" = preds\n",
    "test_df.to_excel(\"./data/processed/test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 96.41999999999999 %; number is 24580\n",
      "organ accuracy is 95.57 %; number is 610\n",
      "Bx-site accuracy is 78.85 %; number is 610\n",
      "sampling method accuracy is 90.82000000000001 %; number is 610\n",
      "diagnosis accuracy is 80.49 %; number is 610\n",
      "size accuracy is 88.33 %; number is 540\n",
      "Greatest_dimension accuracy is 93.7 %; number is 540\n",
      "H_type accuracy is 89.44 %; number is 540\n",
      "H_grade accuracy is 97.59 %; number is 540\n",
      "TF accuracy is 97.22 %; number is 540\n",
      "LV accuracy is 96.11 %; number is 540\n",
      "CM accuracy is 95.19 %; number is 540\n",
      "VPI accuracy is 97.22 %; number is 540\n",
      "EGFR accuracy is 98.89 %; number is 540\n",
      "ALK accuracy is 100.0 %; number is 540\n",
      "ROS1 accuracy is 99.81 %; number is 540\n",
      "BRAF accuracy is 100.0 %; number is 540\n",
      "MET accuracy is 100.0 %; number is 540\n",
      "KRAS accuracy is 100.0 %; number is 540\n",
      "ERBB2 accuracy is 100.0 %; number is 540\n",
      "PIK3CA accuracy is 100.0 %; number is 540\n",
      "NRAS accuracy is 100.0 %; number is 540\n",
      "MEK1 accuracy is 100.0 %; number is 540\n",
      "NTRK accuracy is 100.0 %; number is 540\n",
      "RET accuracy is 100.0 %; number is 540\n",
      "PDL1 accuracy is 92.58999999999999 %; number is 540\n",
      "ver accuracy is 98.7 %; number is 540\n",
      "pT accuracy is 93.7 %; number is 540\n",
      "pN accuracy is 94.07 %; number is 540\n",
      "pM accuracy is 94.63000000000001 %; number is 540\n",
      "pStage accuracy is 89.07000000000001 %; number is 540\n",
      "CK7 accuracy is 97.78 %; number is 540\n",
      "TTF accuracy is 95.37 %; number is 540\n",
      "Napsin accuracy is 97.78 %; number is 540\n",
      "CK20 accuracy is 98.15 %; number is 540\n",
      "P40 accuracy is 97.41 %; number is 540\n",
      "CDX2 accuracy is 99.81 %; number is 540\n",
      "P63 accuracy is 99.26 %; number is 540\n",
      "P16 accuracy is 100.0 %; number is 540\n",
      "cytokeratin accuracy is 99.26 %; number is 540\n",
      "Vimentin accuracy is 100.0 %; number is 540\n",
      "PAX accuracy is 99.44 %; number is 540\n",
      "CD56 accuracy is 99.07000000000001 %; number is 540\n",
      "chromogranin accuracy is 99.26 %; number is 540\n",
      "synaptophysin accuracy is 98.7 %; number is 540\n",
      "GATA3 accuracy is 100.0 %; number is 540\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path, dtype=str)\n",
    "\n",
    "labels = test_df.target_text.values.tolist()\n",
    "preds = test_df.prediction.values.tolist()\n",
    "\n",
    "labels = l.strip() for l in labels\n",
    "preds = p.strip() for p in preds\n",
    "\n",
    "acc = sum(1 for l, p in zip(labels, preds) if l == p) / len(labels)\n",
    "print(f\"Accuracy is {round(acc, 4) * 100} %; number is {len(labels)}\")\n",
    "\n",
    "for target, question in target_to_question.items():\n",
    "    t_labels, t_preds = , \n",
    "    t_labels = row.target_text for row in test_df.itertuples() if question in row.input_text\n",
    "    t_preds = row.prediction for row in test_df.itertuples() if question in row.input_text\n",
    "    acc = sum(1 for l, p in zip(t_labels, t_preds) if l == p) / len(t_labels)\n",
    "    print(f\"{target} accuracy is {round(acc, 4) * 100} %; number is {len(t_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_cat = {\n",
    "    'EGFR': \"18\", \"19\", \"19,20\", \"20\", \"20,21\", \"21\", \"N\", \"U\",\n",
    "    'ALK': \"P\", \"N\", \"U\",\n",
    "    'ROS1': \"P\", \"N\", \"U\",\n",
    "    'BRAF': \"P\", \"N\", \"U\",\n",
    "    'MET': \"P\", \"N\", \"U\",\n",
    "    'KRAS': \"P\", \"N\", \"U\",\n",
    "    'ERBB2': \"P\", \"N\", \"U\",\n",
    "    'PIK3CA': \"P\", \"N\", \"U\",\n",
    "    'NRAS': \"P\", \"N\", \"U\",\n",
    "    'MEK1': \"P\", \"N\", \"U\",\n",
    "    'NTRK': \"P\", \"N\", \"U\",\n",
    "    'RET': \"P\", \"N\", \"U\",\n",
    "    'CK7': \"P\", \"N\", \"U\",\n",
    "    'TTF': \"P\", \"N\", \"U\",\n",
    "    'Napsin': \"P\", \"N\", \"U\",\n",
    "    'CK20': \"P\", \"N\", \"U\",\n",
    "    'P40': \"P\", \"N\", \"U\",\n",
    "    'CDX2': \"P\", \"N\", \"U\",\n",
    "    'P63': \"P\", \"N\", \"U\",\n",
    "    'P16': \"P\", \"N\", \"U\",\n",
    "    'cytokeratin': \"P\", \"N\", \"U\",\n",
    "    'Vimentin': \"P\", \"N\", \"U\",\n",
    "    'PAX': \"P\", \"N\", \"U\",\n",
    "    'CD56': \"P\", \"N\", \"U\",\n",
    "    'chromogranin': \"P\", \"N\", \"U\",\n",
    "    'synaptophysin': \"P\", \"N\", \"U\",\n",
    "    'GATA3': \"P\", \"N\", \"U\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.88      1.00      0.93        14\n",
      "           2       1.00      0.33      0.50         6\n",
      "           3       1.00      1.00      1.00        33\n",
      "           4       0.94      1.00      0.97        44\n",
      "           5       1.00      1.00      1.00       445\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       543\n",
      "   macro avg       0.80      0.72      0.73       543\n",
      "weighted avg       0.99      0.99      0.99       543\n",
      " samples avg       0.99      0.99      0.99       543\n",
      "\n",
      "[[[539   0]\n",
      "  [  1   0]]\n",
      "\n",
      " [[524   2]\n",
      "  [  0  14]]\n",
      "\n",
      " [[534   0]\n",
      "  [  4   2]]\n",
      "\n",
      " [[507   0]\n",
      "  [  0  33]]\n",
      "\n",
      " [[493   3]\n",
      "  [  0  44]]\n",
      "\n",
      " [[ 95   0]\n",
      "  [  1 444]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "df = test_dftest_df\"prefix\" == 'EGFR'\n",
    "labels = df.target_text.values.tolist()\n",
    "preds = df.prediction.values.tolist()\n",
    "\n",
    "EGFR = \"18\", \"19\", \"20\", \"21\", \"N\", \"U\"\n",
    "\n",
    "labels = 1 if e in str(l) else 0 for e in EGFR for l in labels\n",
    "preds = 1 if e in str(l) else 0 for e in EGFR for l in preds\n",
    "\n",
    "cr = classification_report(labels, preds)\n",
    "cm = multilabel_confusion_matrix(labels, preds)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.82      0.80      0.81        40\n",
      "           P       0.95      0.88      0.91       103\n",
      "           U       0.97      0.99      0.98       397\n",
      "\n",
      "    accuracy                           0.96       540\n",
      "   macro avg       0.91      0.89      0.90       540\n",
      "weighted avg       0.95      0.96      0.95       540\n",
      "\n",
      "[[ 32   3   5]\n",
      " [  5  91   7]\n",
      " [  2   2 393]]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "df = test_dftest_df\"prefix\" == 'TTF'\n",
    "labels = df.target_text.values.tolist()\n",
    "preds = df.prediction.values.tolist()\n",
    "\n",
    "preds = \"U\" if p not in \"P\", \"N\", \"U\" else p for p in preds\n",
    "\n",
    "cr = classification_report(labels, preds)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "for target, cat in targets_cat.items():\n",
    "    print(target)\n",
    "    df = test_dftest_df\"prefix\" == target\n",
    "    labels = df.target_text.values.tolist()\n",
    "    preds = df.prediction.values.tolist()\n",
    "    # assert all(p in cat for p in preds), target\n",
    "\n",
    "    cr = classification_report(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(cr)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGFR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.87      1.00      0.93        13\n",
      "       19,20       1.00      1.00      1.00         1\n",
      "          20       0.00      0.00      0.00         3\n",
      "       20,21       1.00      0.50      0.67         2\n",
      "          21       0.97      1.00      0.98        31\n",
      "           N       0.94      1.00      0.97        44\n",
      "           U       1.00      1.00      1.00       445\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.72      0.69      0.69       540\n",
      "weighted avg       0.98      0.99      0.99       540\n",
      "\n",
      "[[  0   0   0   0   0   0   1   0]\n",
      " [  0  13   0   0   0   0   0   0]\n",
      " [  0   0   1   0   0   0   0   0]\n",
      " [  0   2   0   0   0   0   1   0]\n",
      " [  0   0   0   0   1   1   0   0]\n",
      " [  0   0   0   0   0  31   0   0]\n",
      " [  0   0   0   0   0   0  44   0]\n",
      " [  0   0   0   0   0   0   1 444]]\n",
      "ALK\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00        51\n",
      "           P       1.00      1.00      1.00         2\n",
      "           U       1.00      1.00      1.00       487\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[ 51   0   0]\n",
      " [  0   2   0]\n",
      " [  0   0 487]]\n",
      "ROS1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.98      0.99        45\n",
      "           P       0.90      1.00      0.95         9\n",
      "           U       1.00      1.00      1.00       486\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       0.97      0.99      0.98       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[ 44   1   0]\n",
      " [  0   9   0]\n",
      " [  0   0 486]]\n",
      "BRAF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "MET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "KRAS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "ERBB2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "PIK3CA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "NRAS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "MEK1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "NTRK\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "RET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "CK7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      1.00      0.50         2\n",
      "           P       0.97      0.85      0.90        67\n",
      "           U       0.99      1.00      0.99       471\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.76      0.95      0.80       540\n",
      "weighted avg       0.98      0.98      0.98       540\n",
      "\n",
      "[[  2   0   0]\n",
      " [  4  57   6]\n",
      " [  0   2 469]]\n",
      "TTF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.82      0.80      0.81        40\n",
      "           P       0.95      0.88      0.91       103\n",
      "       SP263       0.00      0.00      0.00         0\n",
      "           U       0.97      0.99      0.98       397\n",
      "\n",
      "    accuracy                           0.95       540\n",
      "   macro avg       0.68      0.67      0.68       540\n",
      "weighted avg       0.95      0.95      0.95       540\n",
      "\n",
      "[[ 32   3   0   5]\n",
      " [  5  91   0   7]\n",
      " [  0   0   0   0]\n",
      " [  2   2   1 392]]\n",
      "Napsin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.79      0.86      0.83        22\n",
      "           P       0.90      0.74      0.81        35\n",
      "           U       0.99      1.00      1.00       483\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.89      0.87      0.88       540\n",
      "weighted avg       0.98      0.98      0.98       540\n",
      "\n",
      "[[ 19   3   0]\n",
      " [  5  26   4]\n",
      " [  0   0 483]]\n",
      "CK20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.94      0.92      0.93        48\n",
      "           P       0.67      0.80      0.73         5\n",
      "           U       0.99      0.99      0.99       487\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.86      0.90      0.88       540\n",
      "weighted avg       0.98      0.98      0.98       540\n",
      "\n",
      "[[ 44   0   4]\n",
      " [  0   4   1]\n",
      " [  3   2 482]]\n",
      "P40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.89      0.87      0.88        47\n",
      "           P       0.91      0.80      0.85        25\n",
      "           U       0.99      0.99      0.99       468\n",
      "\n",
      "    accuracy                           0.97       540\n",
      "   macro avg       0.93      0.89      0.91       540\n",
      "weighted avg       0.97      0.97      0.97       540\n",
      "\n",
      "[[ 41   2   4]\n",
      " [  2  20   3]\n",
      " [  3   0 465]]\n",
      "CDX2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.80      1.00      0.89         4\n",
      "           U       1.00      1.00      1.00       536\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       0.90      1.00      0.94       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[  4   0]\n",
      " [  1 535]]\n",
      "P63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      1.00      0.50         1\n",
      "           P       0.50      1.00      0.67         2\n",
      "           U       1.00      0.99      1.00       537\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.61      1.00      0.72       540\n",
      "weighted avg       1.00      0.99      0.99       540\n",
      "\n",
      "[[  1   0   0]\n",
      " [  0   2   0]\n",
      " [  2   2 533]]\n",
      "P16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "cytokeratin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P       0.75      0.90      0.82        10\n",
      "           U       1.00      0.99      1.00       530\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.87      0.95      0.91       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "[[  9   1]\n",
      " [  3 527]]\n",
      "Vimentin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "PAX\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.50      1.00      0.67         3\n",
      "           P       0.00      0.00      0.00         2\n",
      "           U       1.00      1.00      1.00       535\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.50      0.67      0.56       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "[[  3   0   0]\n",
      " [  2   0   0]\n",
      " [  1   0 534]]\n",
      "CD56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.60      0.50      0.55         6\n",
      "           P       1.00      0.89      0.94         9\n",
      "           U       0.99      1.00      1.00       525\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.86      0.80      0.83       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "[[  3   0   3]\n",
      " [  1   8   0]\n",
      " [  1   0 524]]\n",
      "chromogranin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.67      0.67      0.67         6\n",
      "           P       1.00      0.86      0.92         7\n",
      "           U       1.00      1.00      1.00       527\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.89      0.84      0.86       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n",
      "[[  4   0   2]\n",
      " [  1   6   0]\n",
      " [  1   0 526]]\n",
      "synaptophysin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.33      0.25      0.29         4\n",
      "           P       0.70      0.78      0.74         9\n",
      "           U       1.00      1.00      1.00       527\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.68      0.67      0.67       540\n",
      "weighted avg       0.99      0.99      0.99       540\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1   2   1]\n",
      " [  1   7   1]\n",
      " [  1   1 525]]\n",
      "GATA3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "for target, cat in targets_cat.items():\n",
    "    print(target)\n",
    "    df = test_dftest_df\"prefix\" == target\n",
    "    labels = df.target_text.values.tolist()\n",
    "    preds = df.prediction.values.tolist()\n",
    "    # assert all(p in cat for p in preds), target\n",
    "\n",
    "    cr = classification_report(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(cr)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with extracted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "NO_TARGET = \"-\"\n",
    "NO_TEXT = \"NO\"\n",
    "USE_CONTENT = True\n",
    "\n",
    "data_path = {\n",
    "    \"train\": \"./data/processed/train.json\",\n",
    "    \"test\": \"./data/processed/test.json\"\n",
    "}\n",
    "\n",
    "target_to_prefix = json.loads(Path(\"./save/target_to_prefix.json\").read_text())\n",
    "target_to_question = json.loads(Path(\"./save/target_to_question.json\").read_text())\n",
    "soap_targets = json.loads(Path(\"./save/soap_targets.json\").read_text())\n",
    "\n",
    "input_data = defaultdict(list)\n",
    "for split, path in data_path.items():\n",
    "    split_data = json.loads(Path(path).read_text())\n",
    "    for example in split_data:\n",
    "        for target, items in example\"NER_target\".items():\n",
    "            prefix = target_to_prefixtarget\n",
    "            question = target_to_questiontarget\n",
    "            if USE_CONTENT:\n",
    "                if target not in soap_targets and example\"path report\" == \"\":\n",
    "                    continue\n",
    "                input_text = f\"{question} context: {items'content'} {NO_TEXT}\"\n",
    "            else:\n",
    "                if target in soap_targets:\n",
    "                    if example\"soap report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'soap report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        input_text = f\"{question} context: {example'cetology report'} {NO_TEXT}\"\n",
    "                else:\n",
    "                    if example\"path report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'path report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        continue\n",
    "            input_text = re.sub(\"\\s+\", \" \", input_text)\n",
    "            if items\"annotate\" == NO_TARGET:\n",
    "                target_text = NO_TEXT\n",
    "            elif items\"annotate\" == \"P\":\n",
    "                target_text = \"Positive\"\n",
    "            elif items\"annotate\" == \"N\":\n",
    "                target_text = \"Negative\"\n",
    "            elif items\"annotate\" == \"U\":\n",
    "                target_text = \"Unknown\"\n",
    "            else:\n",
    "                target_text = items\"annotate\"\n",
    "            input_datasplit.append(prefix, input_text, target_text)\n",
    "                    \n",
    "\n",
    "train_df = pd.DataFrame(input_data\"train\")\n",
    "train_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "train_df.to_excel(\"./data/processed/train_content.xlsx\", index=False)\n",
    "\n",
    "test_df = pd.DataFrame(input_data\"test\")\n",
    "test_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "test_df.to_excel(\"./data/processed/test_content.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b\n",
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "train_df = pd.read_excel(\"./data/processed/train_content.xlsx\", dtype=str)\n",
    "test_df = pd.read_excel(\"./data/processed/test_content.xlsx\", dtype=str)\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "\n",
    "# Configure the model\n",
    "# General args: https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model\n",
    "# T5 args: https://simpletransformers.ai/docs/t5-model/\n",
    "# If memory problem occurs, set lower max_seq_length or train_batch_size\n",
    "model_args = T5Args()\n",
    "model_args.manual_seed = 1209\n",
    "model_args.max_seq_length = 900\n",
    "model_args.train_batch_size = 4\n",
    "model_args.num_train_epochs = 3\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.n_gpu = 2\n",
    "model_args.output_dir = \"./outputs/t5-with-content/\"\n",
    "model_args.wandb_project = \"lung-cancer\"\n",
    "model_args.wandb_kwargs = {\"name\": \"t5-with-content\"}\n",
    "\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, use_cuda=True)\n",
    "\n",
    "# Make predictions with the model\n",
    "preds = model.predict(to_predict)\n",
    "test_df\"prediction\" = preds\n",
    "test_df.to_excel(model_args.output_dir+\"test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is 95.5 %; number is 24580\n",
      "organ accuracy is 95.89999999999999 %; number is 610\n",
      "Bx-site accuracy is 75.9 %; number is 610\n",
      "sampling method accuracy is 86.72 %; number is 610\n",
      "diagnosis accuracy is 68.52000000000001 %; number is 610\n",
      "size accuracy is 88.33 %; number is 540\n",
      "Greatest_dimension accuracy is 89.44 %; number is 540\n",
      "H_type accuracy is 89.25999999999999 %; number is 540\n",
      "H_grade accuracy is 97.41 %; number is 540\n",
      "TF accuracy is 93.15 %; number is 540\n",
      "LV accuracy is 97.04 %; number is 540\n",
      "CM accuracy is 97.59 %; number is 540\n",
      "VPI accuracy is 99.07000000000001 %; number is 540\n",
      "EGFR accuracy is 98.7 %; number is 540\n",
      "ALK accuracy is 100.0 %; number is 540\n",
      "ROS1 accuracy is 98.7 %; number is 540\n",
      "BRAF accuracy is 100.0 %; number is 540\n",
      "MET accuracy is 100.0 %; number is 540\n",
      "KRAS accuracy is 100.0 %; number is 540\n",
      "ERBB2 accuracy is 100.0 %; number is 540\n",
      "PIK3CA accuracy is 100.0 %; number is 540\n",
      "NRAS accuracy is 100.0 %; number is 540\n",
      "MEK1 accuracy is 100.0 %; number is 540\n",
      "NTRK accuracy is 100.0 %; number is 540\n",
      "RET accuracy is 100.0 %; number is 540\n",
      "PDL1 accuracy is 86.85000000000001 %; number is 540\n",
      "ver accuracy is 99.07000000000001 %; number is 540\n",
      "pT accuracy is 96.3 %; number is 540\n",
      "pN accuracy is 96.3 %; number is 540\n",
      "pM accuracy is 96.48 %; number is 540\n",
      "pStage accuracy is 91.3 %; number is 540\n",
      "CK7 accuracy is 96.3 %; number is 540\n",
      "TTF accuracy is 94.26 %; number is 540\n",
      "Napsin accuracy is 96.67 %; number is 540\n",
      "CK20 accuracy is 97.59 %; number is 540\n",
      "P40 accuracy is 94.26 %; number is 540\n",
      "CDX2 accuracy is 98.89 %; number is 540\n",
      "P63 accuracy is 99.63 %; number is 540\n",
      "P16 accuracy is 100.0 %; number is 540\n",
      "cytokeratin accuracy is 98.15 %; number is 540\n",
      "Vimentin accuracy is 99.63 %; number is 540\n",
      "PAX accuracy is 99.63 %; number is 540\n",
      "CD56 accuracy is 91.85 %; number is 540\n",
      "chromogranin accuracy is 97.04 %; number is 540\n",
      "synaptophysin accuracy is 97.96000000000001 %; number is 540\n",
      "GATA3 accuracy is 100.0 %; number is 540\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "test_excel_path = \"./outputs/t5-with-content/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path, dtype=str)\n",
    "\n",
    "labels = test_df.target_text.values.tolist()\n",
    "preds = test_df.prediction.values.tolist()\n",
    "\n",
    "labels = l.strip() for l in labels\n",
    "preds = p.strip() for p in preds\n",
    "\n",
    "target_to_question = json.loads(Path(\"./save/target_to_question.json\").read_text())\n",
    "\n",
    "acc = sum(1 for l, p in zip(labels, preds) if l == p) / len(labels)\n",
    "print(f\"Accuracy is {round(acc, 4) * 100} %; number is {len(labels)}\")\n",
    "\n",
    "for target, question in target_to_question.items():\n",
    "    t_labels, t_preds = , \n",
    "    t_labels = row.target_text for row in test_df.itertuples() if question in row.input_text\n",
    "    t_preds = row.prediction for row in test_df.itertuples() if question in row.input_text\n",
    "    acc = sum(1 for l, p in zip(t_labels, t_preds) if l == p) / len(t_labels)\n",
    "    print(f\"{target} accuracy is {round(acc, 4) * 100} %; number is {len(t_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "24580\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_excel_path = \"./outputs/t5-with-content/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "df = test_dftest_df\"prefix\" == 'ALK'\n",
    "print(len(df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540\n",
      "24580\n"
     ]
    }
   ],
   "source": [
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "df = test_dftest_df\"prefix\" == 'ALK'\n",
    "print(len(df))\n",
    "print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EGFR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.92      0.85      0.88        13\n",
      "       19,20       0.00      0.00      0.00         1\n",
      "          20       0.75      1.00      0.86         3\n",
      "       20,21       0.00      0.00      0.00         2\n",
      "          21       0.97      0.97      0.97        31\n",
      "    Negative       0.96      1.00      0.98        44\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "     Unknown       1.00      1.00      1.00       445\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.46      0.48      0.47       540\n",
      "weighted avg       0.98      0.99      0.99       540\n",
      "\n",
      "[[  0   0   0   0   0   0   1   0   0   0]\n",
      " [  0  11   0   0   0   0   1   0   0   1]\n",
      " [  0   1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   3   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0  30   0   1   0   0]\n",
      " [  0   0   0   0   0   0  44   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0 445   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0]]\n",
      "ALK\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00        51\n",
      "    Positive       1.00      1.00      1.00         2\n",
      "     Unknown       1.00      1.00      1.00       487\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[ 51   0   0]\n",
      " [  0   2   0]\n",
      " [  0   0 487]]\n",
      "ROS1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.84      0.92        45\n",
      "    Positive       1.00      1.00      1.00         9\n",
      "     Unknown       1.00      1.00      1.00       486\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.75      0.71      0.73       540\n",
      "weighted avg       1.00      0.99      0.99       540\n",
      "\n",
      "[[ 38   0   0   7]\n",
      " [  0   9   0   0]\n",
      " [  0   0 486   0]\n",
      " [  0   0   0   0]]\n",
      "BRAF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "MET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "KRAS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "ERBB2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "PIK3CA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "NRAS\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "MEK1\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "NTRK\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "RET\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "CK7\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         2\n",
      "    Positive       0.78      0.97      0.87        67\n",
      "     Unknown       1.00      0.96      0.98       471\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       540\n",
      "   macro avg       0.70      0.73      0.71       540\n",
      "weighted avg       0.97      0.96      0.97       540\n",
      "\n",
      "[[  2   0   0   0]\n",
      " [  0  65   1   1]\n",
      " [  0  18 453   0]\n",
      " [  0   0   0   0]]\n",
      "TTF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.62      0.72        40\n",
      "    Positive       0.81      0.93      0.86       103\n",
      "     Unknown       0.99      0.98      0.98       397\n",
      "\n",
      "    accuracy                           0.94       540\n",
      "   macro avg       0.89      0.84      0.86       540\n",
      "weighted avg       0.95      0.94      0.94       540\n",
      "\n",
      "[[ 25  14   1]\n",
      " [  4  96   3]\n",
      " [  0   9 388]]\n",
      "Napsin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.93      0.59      0.72        22\n",
      "    Positive       0.69      0.97      0.81        35\n",
      "     Unknown       1.00      0.98      0.99       483\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       540\n",
      "   macro avg       0.66      0.64      0.63       540\n",
      "weighted avg       0.98      0.97      0.97       540\n",
      "\n",
      "[[ 13   7   1   1]\n",
      " [  1  34   0   0]\n",
      " [  0   8 475   0]\n",
      " [  0   0   0   0]]\n",
      "CK20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.97      0.79      0.87        48\n",
      "    Positive       0.38      1.00      0.56         5\n",
      "     Unknown       0.99      0.99      0.99       487\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.78      0.93      0.81       540\n",
      "weighted avg       0.98      0.98      0.98       540\n",
      "\n",
      "[[ 38   6   4]\n",
      " [  0   5   0]\n",
      " [  1   2 484]]\n",
      "P40\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.94      0.72      0.82        47\n",
      "    Positive       0.49      0.88      0.63        25\n",
      "     Unknown       0.99      0.97      0.98       468\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.94       540\n",
      "   macro avg       0.61      0.64      0.61       540\n",
      "weighted avg       0.96      0.94      0.95       540\n",
      "\n",
      "[[ 34   9   4   0]\n",
      " [  2  22   1   0]\n",
      " [  0  14 453   1]\n",
      " [  0   0   0   0]]\n",
      "CDX2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      0.75      0.86         4\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "     Unknown       1.00      0.99      0.99       536\n",
      "\n",
      "    accuracy                           0.99       540\n",
      "   macro avg       0.67      0.58      0.62       540\n",
      "weighted avg       1.00      0.99      0.99       540\n",
      "\n",
      "[[  3   0   1]\n",
      " [  0   0   0]\n",
      " [  0   5 531]]\n",
      "P63\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         1\n",
      "    Positive       0.50      0.50      0.50         2\n",
      "     Unknown       1.00      1.00      1.00       537\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       0.83      0.83      0.83       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[  1   0   0]\n",
      " [  0   1   1]\n",
      " [  0   1 536]]\n",
      "P16\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n",
      "cytokeratin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.50      0.90      0.64        10\n",
      "     Unknown       1.00      0.98      0.99       530\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.75      0.94      0.82       540\n",
      "weighted avg       0.99      0.98      0.98       540\n",
      "\n",
      "[[  9   1]\n",
      " [  9 521]]\n",
      "Vimentin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       0.50      0.50      0.50       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[  0   0]\n",
      " [  2 538]]\n",
      "PAX\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       1.00      1.00      1.00         3\n",
      "    Positive       0.50      0.50      0.50         2\n",
      "     Unknown       1.00      1.00      1.00       535\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       0.83      0.83      0.83       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[  3   0   0]\n",
      " [  0   1   1]\n",
      " [  0   1 534]]\n",
      "CD56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.80      0.67      0.73         6\n",
      "    Positive       0.16      0.89      0.27         9\n",
      "     Unknown       1.00      0.92      0.96       525\n",
      "\n",
      "    accuracy                           0.92       540\n",
      "   macro avg       0.65      0.83      0.65       540\n",
      "weighted avg       0.98      0.92      0.95       540\n",
      "\n",
      "[[  4   2   0]\n",
      " [  1   8   0]\n",
      " [  0  41 484]]\n",
      "chromogranin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.50      0.60         6\n",
      "    Positive       0.35      0.86      0.50         7\n",
      "     Unknown       0.99      0.98      0.99       527\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       540\n",
      "   macro avg       0.52      0.58      0.52       540\n",
      "weighted avg       0.98      0.97      0.98       540\n",
      "\n",
      "[[  3   0   3   0]\n",
      " [  1   6   0   0]\n",
      " [  0  11 515   1]\n",
      " [  0   0   0   0]]\n",
      "synaptophysin\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75         4\n",
      "    Positive       0.47      0.89      0.62         9\n",
      "     Unknown       1.00      0.98      0.99       527\n",
      "        none       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.56      0.66      0.59       540\n",
      "weighted avg       0.99      0.98      0.98       540\n",
      "\n",
      "[[  3   1   0   0]\n",
      " [  1   8   0   0]\n",
      " [  0   8 518   1]\n",
      " [  0   0   0   0]]\n",
      "GATA3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Unknown       1.00      1.00      1.00       540\n",
      "\n",
      "    accuracy                           1.00       540\n",
      "   macro avg       1.00      1.00      1.00       540\n",
      "weighted avg       1.00      1.00      1.00       540\n",
      "\n",
      "[[540]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "targets_cat = {\n",
    "    'EGFR': \"18\", \"19\", \"19\", \"20\", \"20\", \"21\", \"N\", \"U\",\n",
    "    'ALK': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'ROS1': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'BRAF': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'MET': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'KRAS': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'ERBB2': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'PIK3CA': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'NRAS': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'MEK1': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'NTRK': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'RET': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'CK7': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'TTF': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'Napsin': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'CK20': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'P40': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'CDX2': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'P63': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'P16': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'cytokeratin': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'Vimentin': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'PAX': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'CD56': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'chromogranin': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'synaptophysin': \"Positive\", \"Negative\", \"Unknown\",\n",
    "    'GATA3': \"Positive\", \"Negative\", \"Unknown\"\n",
    "}\n",
    "\n",
    "test_excel_path = \"./outputs/t5-with-content/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "for target, cat in targets_cat.items():\n",
    "    print(target)\n",
    "    df = test_dftest_df\"prefix\" == target\n",
    "    labels = df.target_text.values.tolist()\n",
    "    preds = df.prediction.values.tolist()\n",
    "    #assert all(p in cat for p in preds), target\n",
    "\n",
    "    cr = classification_report(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    print(cr)\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_excel_path = \"./data/processed/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "for target, cat in targets_cat.items():\n",
    "    df = test_dftest_df\"prefix\" == target\n",
    "    labels = df.target_text.values.tolist()\n",
    "    preds = df.prediction.values.tolist()\n",
    "\n",
    "    cr = classification_report(labels, preds, labels=cat)\n",
    "    cm = confusion_matrix(labels, preds, labels=cat)\n",
    "    print(cr)\n",
    "    print(cm)\n",
    "    # classification_reports.append(f\"{target}\\n{cr}\\n\")\n",
    "    # confusion_matrics.append(f\"{target}\\n{cm}\\n\")\n",
    "\n",
    "# Path(\"./save/all_classification_report.txt\").write_text(\"\\n\".join(classification_reports))\n",
    "# Path(\"./save/all_confusion_matrix.txt\").write_text(\"\\n\".join(confusion_matrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       1.00      0.86      0.92        14\n",
      "           2       1.00      0.67      0.80         6\n",
      "           3       1.00      0.94      0.97        33\n",
      "           4       0.96      1.00      0.98        44\n",
      "           5       1.00      1.00      1.00       445\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       543\n",
      "   macro avg       0.83      0.74      0.78       543\n",
      "weighted avg       0.99      0.99      0.99       543\n",
      " samples avg       0.99      0.99      0.99       543\n",
      "\n",
      "[[[539   0]\n",
      "  [  1   0]]\n",
      "\n",
      " [[526   0]\n",
      "  [  2  12]]\n",
      "\n",
      " [[534   0]\n",
      "  [  2   4]]\n",
      "\n",
      " [[507   0]\n",
      "  [  2  31]]\n",
      "\n",
      " [[494   2]\n",
      "  [  0  44]]\n",
      "\n",
      " [[ 93   2]\n",
      "  [  0 445]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chchen/anaconda3/envs/simple-transformers/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, multilabel_confusion_matrix\n",
    "\n",
    "test_excel_path = \"./outputs/t5-with-content/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "classification_reports = \n",
    "confusion_matrics = \n",
    "\n",
    "df = test_dftest_df\"prefix\" == 'EGFR'\n",
    "labels = df.target_text.values.tolist()\n",
    "preds = df.prediction.values.tolist()\n",
    "\n",
    "preds = \"U\" if p in \"Positive\", \"none\" else p for p in preds\n",
    "\n",
    "EGFR = \"18\", \"19\", \"20\", \"21\", \"N\", \"U\"\n",
    "\n",
    "labels = 1 if e in str(l) else 0 for e in EGFR for l in labels\n",
    "preds = 1 if e in str(l) else 0 for e in EGFR for l in preds\n",
    "\n",
    "cr = classification_report(labels, preds)\n",
    "cm = multilabel_confusion_matrix(labels, preds)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.75      0.75      0.75         4\n",
      "    Positive       0.47      0.89      0.62         9\n",
      "     Unknown       1.00      0.98      0.99       527\n",
      "\n",
      "    accuracy                           0.98       540\n",
      "   macro avg       0.74      0.87      0.79       540\n",
      "weighted avg       0.99      0.98      0.98       540\n",
      "\n",
      "[[  3   1   0]\n",
      " [  1   8   0]\n",
      " [  0   8 519]]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "test_excel_path = \"./outputs/t5-with-content/test.xlsx\"\n",
    "test_df = pd.read_excel(test_excel_path)\n",
    "\n",
    "df = test_dftest_df\"prefix\" == 'synaptophysin'\n",
    "labels = df.target_text.values.tolist()\n",
    "preds = df.prediction.values.tolist()\n",
    "\n",
    "preds = \"Unknown\" if p == \"none\" else p for p in preds\n",
    "\n",
    "\n",
    "cr = classification_report(labels, preds)\n",
    "cm = confusion_matrix(labels, preds)\n",
    "print(cr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with 860 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = \"./data/raw/merge_keycontent.json\"\n",
    "data = json.loads(Path(data_path).read_text())\n",
    "\n",
    "eval_path = \"./data/raw/path_eval_102.json\"\n",
    "eval_label = json.loads(Path(eval_path).read_text())\n",
    "\n",
    "train = datastr(i) for i in range(1, 861)\n",
    "eval = dataidx for idx in eval_label.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'soap report': 'Lung, upper lobe, left, CT-guided biopsy, adenocarcinoma/n/n',\n",
       " 'path report': 'The specimen submitted consists of three tissue fragments measuring up to 1.0 x 0.1 x 0.1 cm in size, fixed in formalin./n/nGrossly, they are gray, soft, and cord-like./n/nAll for section./n/nMicroscopically, it shows a picture of adenocarcinoma arranged in acinar pattern and infiltrating pattern. The carcinoma cells display mild to moderate nuclear pleomorphism. By immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       " 'cetology report': '',\n",
       " 'NER_target': {'organ': {'content': 'Lung, upper lobe, left, CT-guided biopsy, adenocarcinoma\\n\\n',\n",
       "   'annotate': 'Lung'},\n",
       "  'Bx-site': {'content': 'Lung, upper lobe, left, CT-guided biopsy, adenocarcinoma\\n\\n',\n",
       "   'annotate': 'LUL'},\n",
       "  'sampling method': {'content': 'Lung, upper lobe, left, CT-guided biopsy, adenocarcinoma\\n\\n',\n",
       "   'annotate': 'CT-guided biopsy'},\n",
       "  'diagnosis': {'content': 'Lung, upper lobe, left, CT-guided biopsy, adenocarcinoma\\n\\n',\n",
       "   'annotate': 'ADC-Adenocarcinoma'},\n",
       "  'size': {'content': 'without content', 'annotate': '-'},\n",
       "  'Greatest_dimension': {'content': 'without content', 'annotate': '-'},\n",
       "  'H_type': {'content': 'without content', 'annotate': '-'},\n",
       "  'H_grade': {'content': 'without content', 'annotate': '-'},\n",
       "  'TF': {'content': 'without content', 'annotate': '-'},\n",
       "  'LV': {'content': 'without content', 'annotate': '-'},\n",
       "  'CM': {'content': 'without content', 'annotate': '-'},\n",
       "  'VPI': {'content': 'without content', 'annotate': '-'},\n",
       "  'EGFR': {'content': 'without content', 'annotate': 'U'},\n",
       "  'ALK': {'content': 'without content', 'annotate': 'U'},\n",
       "  'ROS1': {'content': 'without content', 'annotate': 'U'},\n",
       "  'BRAF': {'content': 'without content', 'annotate': 'U'},\n",
       "  'MET': {'content': 'without content', 'annotate': 'U'},\n",
       "  'KRAS': {'content': 'without content', 'annotate': 'U'},\n",
       "  'ERBB2': {'content': 'without content', 'annotate': 'U'},\n",
       "  'PIK3CA': {'content': 'without content', 'annotate': 'U'},\n",
       "  'NRAS': {'content': 'without content', 'annotate': 'U'},\n",
       "  'MEK1': {'content': 'without content', 'annotate': 'U'},\n",
       "  'NTRK': {'content': 'without content', 'annotate': 'U'},\n",
       "  'RET': {'content': 'without content', 'annotate': 'U'},\n",
       "  'PDL1': {'content': 'without content', 'annotate': '-'},\n",
       "  'ver': {'content': 'without content', 'annotate': '-'},\n",
       "  'pT': {'content': 'without content', 'annotate': '-'},\n",
       "  'pN': {'content': 'without content', 'annotate': '-'},\n",
       "  'pM': {'content': 'without content', 'annotate': '-'},\n",
       "  'pStage': {'content': 'without content', 'annotate': '-'},\n",
       "  'CK7': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'P'},\n",
       "  'TTF': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'P'},\n",
       "  'Napsin': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'P'},\n",
       "  'CK20': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'P40': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'CDX2': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'P63': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'P16': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'cytokeratin': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'Vimentin': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'PAX': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'CD56': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'chromogranin': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'synaptophysin': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'},\n",
       "  'GATA3': {'content': 'immunostains, the carcinoma is positive for CK7, TTF-1, and napsin A, indicating lung origin.',\n",
       "   'annotate': 'U'}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "NO_TARGET = \"-\"\n",
    "NO_TEXT = \"none\"\n",
    "USE_CONTENT = True\n",
    "\n",
    "target_to_prefix = json.loads(Path(\"./save/target_to_prefix.json\").read_text())\n",
    "target_to_question = json.loads(Path(\"./save/target_to_question.json\").read_text())\n",
    "soap_targets = json.loads(Path(\"./save/soap_targets.json\").read_text())\n",
    "\n",
    "# Load data and get train and eval data\n",
    "data_path = \"./data/raw/merge_keycontent.json\"\n",
    "data = json.loads(Path(data_path).read_text())\n",
    "\n",
    "eval_path = \"./data/raw/path_eval_102.json\"\n",
    "eval_label = json.loads(Path(eval_path).read_text())\n",
    "\n",
    "train = datastr(i) for i in range(1, 861)\n",
    "eval = dataidx for idx in eval_label.keys()\n",
    "\n",
    "dataset = {\"train\": train, \"eval\": eval}\n",
    "\n",
    "# Transform T5 format\n",
    "input_data = defaultdict(list)\n",
    "for split, split_data in dataset.items():\n",
    "    for example in split_data:\n",
    "        for target, items in example\"NER_target\".items():\n",
    "            prefix = target_to_prefixtarget\n",
    "            question = target_to_questiontarget\n",
    "            if USE_CONTENT:\n",
    "                if target not in soap_targets and example\"path report\" == \"\":\n",
    "                    continue\n",
    "                input_text = f\"{question} context: {items'content'} {NO_TEXT}\"\n",
    "            else:\n",
    "                if target in soap_targets:\n",
    "                    if example\"soap report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'soap report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        input_text = f\"{question} context: {example'cetology report'} {NO_TEXT}\"\n",
    "                else:\n",
    "                    if example\"path report\" != \"\":\n",
    "                        input_text = f\"{question} context: {example'path report'} {NO_TEXT}\"\n",
    "                    else:\n",
    "                        continue\n",
    "            input_text = re.sub(\"\\s+\", \" \", input_text)\n",
    "            if items\"annotate\" == NO_TARGET:\n",
    "                target_text = NO_TEXT\n",
    "            elif items\"annotate\" == \"P\":\n",
    "                target_text = \"Positive\"\n",
    "            elif items\"annotate\" == \"N\":\n",
    "                target_text = \"Negative\"\n",
    "            elif items\"annotate\" == \"U\":\n",
    "                target_text = \"Unknown\"\n",
    "            else:\n",
    "                target_text = items\"annotate\"\n",
    "            input_datasplit.append(prefix, input_text, target_text)\n",
    "                    \n",
    "\n",
    "train_df = pd.DataFrame(input_data\"train\")\n",
    "train_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "train_df.to_excel(\"./data/processed/train_860.xlsx\", index=False)\n",
    "\n",
    "test_df = pd.DataFrame(input_data\"eval\")\n",
    "test_df.columns = \"prefix\", \"input_text\", \"target_text\"\n",
    "test_df.to_excel(\"./data/processed/eval_860.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://towardsdatascience.com/the-guide-to-multi-tasking-with-the-t5-transformer-90c70a08837b\n",
    "import logging\n",
    "import pandas as pd\n",
    "from simpletransformers.t5 import T5Model, T5Args\n",
    "\n",
    "\n",
    "train_df = pd.read_excel(\"./data/processed/train_860.xlsx\", dtype=str)\n",
    "test_df = pd.read_excel(\"./data/processed/eval_860.xlsx\", dtype=str)\n",
    "to_predict = f\"{row.prefix}: {row.input_text}\" for row in test_df.itertuples()\n",
    "\n",
    "# Configure the model\n",
    "# General args: https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model\n",
    "# T5 args: https://simpletransformers.ai/docs/t5-model/\n",
    "# If memory problem occurs, set lower max_seq_length or train_batch_size\n",
    "model_args = T5Args()\n",
    "model_args.manual_seed = 1209\n",
    "model_args.max_seq_length = 900\n",
    "model_args.train_batch_size = 4\n",
    "model_args.num_train_epochs = 5\n",
    "model_args.use_multiprocessing = False\n",
    "model_args.fp16 = False\n",
    "model_args.save_steps = -1\n",
    "model_args.save_model_every_epoch = False\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.n_gpu = 2\n",
    "model_args.output_dir = \"./outputs/t5-with-content-860/\"\n",
    "model_args.wandb_project = \"lung-cancer\"\n",
    "model_args.wandb_kwargs = {\"name\": \"t5-with-content-860\"}\n",
    "\n",
    "model = T5Model(\"t5\", \"t5-base\", args=model_args)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_df, use_cuda=True)\n",
    "\n",
    "# Make predictions with the model\n",
    "preds = model.predict(to_predict)\n",
    "test_df\"prediction\" = preds\n",
    "test_df.to_excel(model_args.output_dir+\"test.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "372916"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "target_to_question = json.loads(Path(\"./save/target_to_question.json\").read_text())\n",
    "test = pd.read_excel(\"./outputs/t5-with-content-860/test.xlsx\")\n",
    "\n",
    "def get_target(text):\n",
    "    for target, question in target_to_question.items():\n",
    "        if question in text:\n",
    "            return target\n",
    "\n",
    "test\"target\" = test\"input_text\".apply(get_target)\n",
    "\n",
    "case_id = \n",
    "i = 860\n",
    "for row in test.itertuples():\n",
    "    if row.target == \"organ\":\n",
    "        i += 1\n",
    "    case_id.append(str(i))\n",
    "test\"ID\" = case_id\n",
    "\n",
    "def transform_prediction(text):\n",
    "    if text == \"Positive\":\n",
    "        return \"positive\"\n",
    "    elif text == \"Negative\":\n",
    "        return \"negative\"\n",
    "    elif text == \"Unknown\":\n",
    "        return \"U\"\n",
    "    elif text == \"none\":\n",
    "        return \"-\"\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "test\"prediction\" = test\"prediction\".apply(transform_prediction)\n",
    "\n",
    "eval_path = \"./data/raw/path_eval_102.json\"\n",
    "eval_label = json.loads(Path(eval_path).read_text())\n",
    "\n",
    "for case_id, items in eval_label.items():\n",
    "    for target, labels in items.items():\n",
    "        labels\"t5\" = \"-\"\n",
    "\n",
    "for row in test.itertuples():\n",
    "    if row.prediction != \"-\":\n",
    "        if row.target == \"TTF\":\n",
    "            eval_labelrow.ID\"TTF-1\"\"t5\" = row.prediction\n",
    "        elif row.target == \"Napsin\":\n",
    "            eval_labelrow.ID\"NapsinUA\"\"t5\" = row.prediction\n",
    "        elif row.target == \"cytokeratin\":\n",
    "            eval_labelrow.ID\"cytokeratin (AE1/AE3)\"\"t5\" = row.prediction\n",
    "        elif row.target == \"PAX\":\n",
    "            eval_labelrow.ID\"PAXU8\"\"t5\" = row.prediction\n",
    "        elif row.target == \"chromogranin\":\n",
    "            eval_labelrow.ID\"chromogranin-A\"\"t5\" = row.prediction\n",
    "        elif row.target == \"size\":\n",
    "            eval_labelrow.ID\"tumor_size\"\"t5\" = row.prediction\n",
    "        elif row.target == \"Greatest_dimension\":\n",
    "            eval_labelrow.ID\"Greatest dimension\"\"t5\" = row.prediction\n",
    "        elif row.target == \"LV\":\n",
    "            eval_labelrow.ID\"LV_invasion\"\"t5\" = row.prediction\n",
    "        elif row.target == \"ver\":\n",
    "            eval_labelrow.ID\"version\"\"t5\" = row.prediction\n",
    "        elif row.target == \"H_grade\":\n",
    "            eval_labelrow.ID\"Hgrade\"\"t5\" = row.prediction\n",
    "        elif row.target == \"TF\":\n",
    "            eval_labelrow.ID\"Tumor_Focality\"\"t5\" = row.prediction\n",
    "        elif row.target == \"H_type\":\n",
    "            eval_labelrow.ID\"Htype\"\"t5\" = row.prediction\n",
    "        elif row.target == \"CM\":\n",
    "            eval_labelrow.ID\"closest_margin\"\"t5\" = row.prediction\n",
    "        else:\n",
    "            eval_labelrow.IDrow.target\"t5\" = row.prediction\n",
    "\n",
    "Path(eval_path).write_text(json.dumps(eval_label, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NapsinUA\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         4\n",
      "           P       1.00      1.00      1.00         1\n",
      "           U       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 4  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 97]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.75      0.75      0.75         4\n",
      "           P       0.50      1.00      0.67         1\n",
      "           U       0.99      0.98      0.98        97\n",
      "\n",
      "    accuracy                           0.97       102\n",
      "   macro avg       0.75      0.91      0.80       102\n",
      "weighted avg       0.98      0.97      0.97       102\n",
      "\n",
      "[[ 3  0  1]\n",
      " [ 0  1  0]\n",
      " [ 1  1 95]]\n",
      "version\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "92.16\n",
      "ALK\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         5\n",
      "           P       1.00      1.00      1.00         1\n",
      "           U       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 5  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 96]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         5\n",
      "           P       1.00      1.00      1.00         1\n",
      "           U       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 5  0  0]\n",
      " [ 0  1  0]\n",
      " [ 0  0 96]]\n",
      "pM\n",
      "rule\n",
      "93.14\n",
      "t5\n",
      "90.2\n",
      "cytokeratin (AE1/AE3)\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P       1.00      0.80      0.89         5\n",
      "           U       0.99      1.00      0.99        97\n",
      "\n",
      "    accuracy                           0.99       102\n",
      "   macro avg       0.99      0.90      0.94       102\n",
      "weighted avg       0.99      0.99      0.99       102\n",
      "\n",
      "[[ 4  1]\n",
      " [ 0 97]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P       0.40      0.80      0.53         5\n",
      "           U       0.99      0.94      0.96        97\n",
      "\n",
      "    accuracy                           0.93       102\n",
      "   macro avg       0.69      0.87      0.75       102\n",
      "weighted avg       0.96      0.93      0.94       102\n",
      "\n",
      "[[ 4  1]\n",
      " [ 6 91]]\n",
      "Vimentin\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.50      0.67         2\n",
      "           P       1.00      1.00      1.00         2\n",
      "           U       0.99      1.00      0.99        98\n",
      "\n",
      "    accuracy                           0.99       102\n",
      "   macro avg       1.00      0.83      0.89       102\n",
      "weighted avg       0.99      0.99      0.99       102\n",
      "\n",
      "[[ 1  0  1]\n",
      " [ 0  2  0]\n",
      " [ 0  0 98]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.00      0.00      0.00         2\n",
      "           P       1.00      1.00      1.00         2\n",
      "           U       0.98      1.00      0.99        98\n",
      "\n",
      "    accuracy                           0.98       102\n",
      "   macro avg       0.66      0.67      0.66       102\n",
      "weighted avg       0.96      0.98      0.97       102\n",
      "\n",
      "[[ 0  0  2]\n",
      " [ 0  2  0]\n",
      " [ 0  0 98]]\n",
      "CD56\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.57      0.73         7\n",
      "           P       0.50      1.00      0.67         2\n",
      "           U       0.99      1.00      0.99        93\n",
      "\n",
      "    accuracy                           0.97       102\n",
      "   macro avg       0.83      0.86      0.80       102\n",
      "weighted avg       0.98      0.97      0.97       102\n",
      "\n",
      "[[ 4  2  1]\n",
      " [ 0  2  0]\n",
      " [ 0  0 93]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.43      0.60         7\n",
      "           P       0.67      1.00      0.80         2\n",
      "           U       0.97      1.00      0.98        93\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.88      0.81      0.79       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n",
      "[[ 3  1  3]\n",
      " [ 0  2  0]\n",
      " [ 0  0 93]]\n",
      "EGFR\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      1.00      1.00         6\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00        86\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       104\n",
      "   macro avg       0.83      0.83      0.83       104\n",
      "weighted avg       1.00      1.00      1.00       104\n",
      " samples avg       1.00      1.00      1.00       104\n",
      "\n",
      "[[[102   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[ 96   0]\n",
      "  [  0   6]]\n",
      "\n",
      " [[ 99   0]\n",
      "  [  0   3]]\n",
      "\n",
      " [[ 98   0]\n",
      "  [  0   4]]\n",
      "\n",
      " [[ 97   0]\n",
      "  [  0   5]]\n",
      "\n",
      " [[ 16   0]\n",
      "  [  0  86]]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.33      0.50         6\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       1.00      1.00      1.00         4\n",
      "           4       0.50      1.00      0.67         5\n",
      "           5       1.00      1.00      1.00        86\n",
      "\n",
      "   micro avg       0.95      0.93      0.94       104\n",
      "   macro avg       0.58      0.56      0.53       104\n",
      "weighted avg       0.95      0.93      0.93       104\n",
      " samples avg       0.95      0.94      0.94       104\n",
      "\n",
      "[[[102   0]\n",
      "  [  0   0]]\n",
      "\n",
      " [[ 96   0]\n",
      "  [  4   2]]\n",
      "\n",
      " [[ 99   0]\n",
      "  [  3   0]]\n",
      "\n",
      " [[ 98   0]\n",
      "  [  0   4]]\n",
      "\n",
      " [[ 92   5]\n",
      "  [  0   5]]\n",
      "\n",
      " [[ 16   0]\n",
      "  [  0  86]]]\n",
      "ROS1\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "RET\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "tumor_size\n",
      "rule\n",
      "94.12\n",
      "t5\n",
      "82.35\n",
      "P16\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           P       0.00      0.00      0.00         0\n",
      "           U       1.00      0.98      0.99       102\n",
      "\n",
      "    accuracy                           0.98       102\n",
      "   macro avg       0.50      0.49      0.50       102\n",
      "weighted avg       1.00      0.98      0.99       102\n",
      "\n",
      "[[  0   0]\n",
      " [  2 100]]\n",
      "Tumor_Focality\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "82.35\n",
      "Hgrade\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "94.12\n",
      "PIK3CA\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "VPI\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "100.0\n",
      "BRAF\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "GATA3\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.67      0.80         3\n",
      "           P       0.00      0.00      0.00         3\n",
      "           U       0.96      1.00      0.98        96\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.65      0.56      0.59       102\n",
      "weighted avg       0.93      0.96      0.95       102\n",
      "\n",
      "[[ 2  0  1]\n",
      " [ 0  0  3]\n",
      " [ 0  0 96]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         3\n",
      "           P       1.00      1.00      1.00         3\n",
      "           U       1.00      1.00      1.00        96\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 3  0  0]\n",
      " [ 0  3  0]\n",
      " [ 0  0 96]]\n",
      "PDL1\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "100.0\n",
      "CK20\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.82      0.90        17\n",
      "           P       0.75      0.75      0.75         4\n",
      "           U       0.96      1.00      0.98        81\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.90      0.86      0.88       102\n",
      "weighted avg       0.96      0.96      0.96       102\n",
      "\n",
      "[[14  1  2]\n",
      " [ 0  3  1]\n",
      " [ 0  0 81]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.94      0.94      0.94        17\n",
      "           P       1.00      0.75      0.86         4\n",
      "           U       0.98      0.99      0.98        81\n",
      "\n",
      "    accuracy                           0.97       102\n",
      "   macro avg       0.97      0.89      0.93       102\n",
      "weighted avg       0.97      0.97      0.97       102\n",
      "\n",
      "[[16  0  1]\n",
      " [ 0  3  1]\n",
      " [ 1  0 80]]\n",
      "chromogranin-A\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.14      0.25         7\n",
      "           P       0.33      0.67      0.44         3\n",
      "           U       0.97      1.00      0.98        92\n",
      "\n",
      "    accuracy                           0.93       102\n",
      "   macro avg       0.77      0.60      0.56       102\n",
      "weighted avg       0.95      0.93      0.92       102\n",
      "\n",
      "[[ 1  4  2]\n",
      " [ 0  2  1]\n",
      " [ 0  0 92]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.71      0.83         7\n",
      "           P       1.00      0.33      0.50         3\n",
      "           U       0.96      1.00      0.98        92\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.99      0.68      0.77       102\n",
      "weighted avg       0.96      0.96      0.95       102\n",
      "\n",
      "[[ 5  0  2]\n",
      " [ 0  1  2]\n",
      " [ 0  0 92]]\n",
      "ERBB2\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "diagnosis\n",
      "rule\n",
      "76.47\n",
      "t5\n",
      "61.76\n",
      "Greatest dimension\n",
      "rule\n",
      "94.12\n",
      "t5\n",
      "91.18\n",
      "NTRK\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "MEK1\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "PAXU8\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.00      0.00      0.00         0\n",
      "           U       1.00      0.99      1.00       102\n",
      "\n",
      "    accuracy                           0.99       102\n",
      "   macro avg       0.50      0.50      0.50       102\n",
      "weighted avg       1.00      0.99      1.00       102\n",
      "\n",
      "[[  0   0]\n",
      " [  1 101]]\n",
      "KRAS\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "pStage\n",
      "rule\n",
      "90.2\n",
      "t5\n",
      "86.27\n",
      "LV_invasion\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "97.06\n",
      "pT\n",
      "rule\n",
      "92.16\n",
      "t5\n",
      "91.18\n",
      "pN\n",
      "rule\n",
      "94.12\n",
      "t5\n",
      "96.08\n",
      "Bx-site\n",
      "rule\n",
      "75.49\n",
      "t5\n",
      "56.86\n",
      "P40\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.70      0.82        10\n",
      "           P       0.40      0.67      0.50         3\n",
      "           U       0.99      1.00      0.99        89\n",
      "\n",
      "    accuracy                           0.96       102\n",
      "   macro avg       0.80      0.79      0.77       102\n",
      "weighted avg       0.97      0.96      0.96       102\n",
      "\n",
      "[[ 7  3  0]\n",
      " [ 0  2  1]\n",
      " [ 0  0 89]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00        10\n",
      "           P       1.00      0.67      0.80         3\n",
      "           U       0.99      1.00      0.99        89\n",
      "\n",
      "    accuracy                           0.99       102\n",
      "   macro avg       1.00      0.89      0.93       102\n",
      "weighted avg       0.99      0.99      0.99       102\n",
      "\n",
      "[[10  0  0]\n",
      " [ 0  2  1]\n",
      " [ 0  0 89]]\n",
      "Htype\n",
      "rule\n",
      "92.16\n",
      "t5\n",
      "87.25\n",
      "organ\n",
      "rule\n",
      "86.27\n",
      "t5\n",
      "83.33\n",
      "closest_margin\n",
      "rule\n",
      "100.0\n",
      "t5\n",
      "85.29\n",
      "CDX2\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "P63\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         3\n",
      "           P       1.00      1.00      1.00         2\n",
      "           U       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 3  0  0]\n",
      " [ 0  2  0]\n",
      " [ 0  0 97]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         3\n",
      "           P       1.00      1.00      1.00         2\n",
      "           U       1.00      1.00      1.00        97\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[ 3  0  0]\n",
      " [ 0  2  0]\n",
      " [ 0  0 97]]\n",
      "MET\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "sampling method\n",
      "rule\n",
      "95.1\n",
      "t5\n",
      "83.33\n",
      "TTF-1\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.61      0.73      0.67        15\n",
      "           P       0.81      0.57      0.67        23\n",
      "           U       0.94      1.00      0.97        64\n",
      "\n",
      "    accuracy                           0.86       102\n",
      "   macro avg       0.79      0.77      0.77       102\n",
      "weighted avg       0.86      0.86      0.86       102\n",
      "\n",
      "[[11  3  1]\n",
      " [ 7 13  3]\n",
      " [ 0  0 64]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.92      0.80      0.86        15\n",
      "           P       0.83      0.83      0.83        23\n",
      "           U       0.92      0.94      0.93        64\n",
      "  Un Unknown       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       102\n",
      "   macro avg       0.67      0.64      0.65       102\n",
      "weighted avg       0.90      0.89      0.90       102\n",
      "\n",
      "[[12  1  2  0]\n",
      " [ 1 19  3  0]\n",
      " [ 0  3 60  1]\n",
      " [ 0  0  0  0]]\n",
      "CK7\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       0.25      0.50      0.33         2\n",
      "           P       0.94      0.75      0.83        20\n",
      "           U       0.98      1.00      0.99        80\n",
      "\n",
      "    accuracy                           0.94       102\n",
      "   macro avg       0.72      0.75      0.72       102\n",
      "weighted avg       0.95      0.94      0.94       102\n",
      "\n",
      "[[ 1  1  0]\n",
      " [ 3 15  2]\n",
      " [ 0  0 80]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      1.00      1.00         2\n",
      "           P       0.90      0.95      0.93        20\n",
      "           U       0.99      0.97      0.98        80\n",
      "\n",
      "    accuracy                           0.97       102\n",
      "   macro avg       0.96      0.97      0.97       102\n",
      "weighted avg       0.97      0.97      0.97       102\n",
      "\n",
      "[[ 2  0  0]\n",
      " [ 0 19  1]\n",
      " [ 0  2 78]]\n",
      "synaptophysin\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.38      0.55         8\n",
      "           P       0.20      0.50      0.29         2\n",
      "           U       0.98      1.00      0.99        92\n",
      "\n",
      "    accuracy                           0.94       102\n",
      "   macro avg       0.73      0.62      0.61       102\n",
      "weighted avg       0.97      0.94      0.94       102\n",
      "\n",
      "[[ 3  4  1]\n",
      " [ 0  1  1]\n",
      " [ 0  0 92]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N       1.00      0.75      0.86         8\n",
      "           P       1.00      0.50      0.67         2\n",
      "           U       0.97      1.00      0.98        92\n",
      "\n",
      "    accuracy                           0.97       102\n",
      "   macro avg       0.99      0.75      0.84       102\n",
      "weighted avg       0.97      0.97      0.97       102\n",
      "\n",
      "[[ 6  0  2]\n",
      " [ 0  1  1]\n",
      " [ 0  0 92]]\n",
      "NRAS\n",
      "rule\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n",
      "t5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           U       1.00      1.00      1.00       102\n",
      "\n",
      "    accuracy                           1.00       102\n",
      "   macro avg       1.00      1.00      1.00       102\n",
      "weighted avg       1.00      1.00      1.00       102\n",
      "\n",
      "[[102]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "correction = {\n",
    "    \"TTF-1\": \"TTF\",\n",
    "    \"NapsinUA\": \"Napsin\",\n",
    "    \"cytokeratin (AE1/AE3)\": \"cytokeratin\",\n",
    "    \"PAXU8\": \"PAX\",\n",
    "    \"chromogranin-A\": \"chromogranin\",\n",
    "    \"tumor_size\": \"size\",\n",
    "    \"Greatest dimension\": \"Greatest_dimension\",\n",
    "    \"LV_invasion\": \"LV\",\n",
    "    \"version\": \"ver\",\n",
    "    \"Hgrade\": \"H_grade\",\n",
    "    \"Tumor_Focality\": \"TF\",\n",
    "    \"Htype\": \"H_type\",\n",
    "    \"closest_margin\": \"CM\"\n",
    "}\n",
    "\n",
    "ner_task = \n",
    "    \"organ\", \"Bx-site\", \"sampling method\", \"diagnosis\", \"tumor_size\", \n",
    "    \"Greatest dimension\", \"Htype\", \"Hgrade\", \"Tumor_Focality\", \"LV_invasion\", \"closest_margin\", \n",
    "    \"VPI\", \"PDL1\", \"version\", \"pT\", \"pN\", \"pM\", \"pStage\"\n",
    "\n",
    "\n",
    "mt_task = \"EGFR\"\n",
    "EGFR = \"18\", \"19\", \"20\", \"21\", \"N\", \"U\"\n",
    "\n",
    "def check_ner(text):\n",
    "    if text == \"U\":\n",
    "        return \"-\"\n",
    "    return text\n",
    "\n",
    "def check_cls(text):\n",
    "    if text == \"-\":\n",
    "        return \"U\"\n",
    "    elif text == \"positive\":\n",
    "        return \"P\"\n",
    "    elif text == \"negative\":\n",
    "        return \"N\"\n",
    "    return text\n",
    "\n",
    "def check_mt(text):\n",
    "    if text == \"-\":\n",
    "        text = \"U\"\n",
    "    elif text == \"negative\":\n",
    "        text = \"N\"\n",
    "    elif text == \"positive\":\n",
    "        text = \"U\"\n",
    "    text = str(text)\n",
    "    vector = 1 if e in text else 0 for e in EGFR\n",
    "    return vector\n",
    "\n",
    "def ner_evaluation(y_true, y_pred):\n",
    "    y_true = check_ner(y) for y in y_true\n",
    "    y_pred = check_ner(y) for y in y_pred\n",
    "    acc = sum(1 for t, p in zip(y_true, y_pred) if t == p) / len(y_true)\n",
    "    print(round(acc*100, 2))\n",
    "\n",
    "def cls_evaluation(y_true, y_pred):\n",
    "    y_true = check_cls(y) for y in y_true\n",
    "    y_pred = check_cls(y) for y in y_pred\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "def mt_evaluation(y_true, y_pred):\n",
    "    y_true = check_mt(y) for y in y_true\n",
    "    y_pred = check_mt(y) for y in y_pred\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(multilabel_confusion_matrix(y_true, y_pred))\n",
    "\n",
    "eval_path = \"./data/raw/path_eval_102.json\"\n",
    "eval_label = json.loads(Path(eval_path).read_text())\n",
    "\n",
    "targets = list(set(tar for case_id, items in eval_label.items() for tar in items.keys()))\n",
    "\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    answer = itemstarget\"Ans\" for case_id, items in eval_label.items()\n",
    "    rule = itemstarget\"Rule\" for case_id, items in eval_label.items()\n",
    "    t5 = itemstarget\"t5\" for case_id, items in eval_label.items()\n",
    "    if target in ner_task:\n",
    "        print(\"rule\")\n",
    "        ner_evaluation(answer, rule)\n",
    "        print(\"t5\")\n",
    "        ner_evaluation(answer, t5)\n",
    "    elif target == mt_task:\n",
    "        print(\"rule\")\n",
    "        mt_evaluation(answer, rule)\n",
    "        print(\"t5\")\n",
    "        mt_evaluation(answer, t5)\n",
    "    else:\n",
    "        print(\"rule\")\n",
    "        cls_evaluation(answer, rule)\n",
    "        print(\"t5\")\n",
    "        cls_evaluation(answer, t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the target position "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>part_num</th>\n",
       "      <th>Chat No.</th>\n",
       "      <th>path_soap</th>\n",
       "      <th>path_report</th>\n",
       "      <th>Cytology_report</th>\n",
       "      <th>organ</th>\n",
       "      <th>Bx-site</th>\n",
       "      <th>operation</th>\n",
       "      <th>Htype</th>\n",
       "      <th>tumor_size</th>\n",
       "      <th>Greatest dimension</th>\n",
       "      <th>Tumor_Focality</th>\n",
       "      <th>LV_invasion</th>\n",
       "      <th>closest_margin</th>\n",
       "      <th>version</th>\n",
       "      <th>pT</th>\n",
       "      <th>pN</th>\n",
       "      <th>pM</th>\n",
       "      <th>pStage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>T1_1</td>\n",
       "      <td>17956402</td>\n",
       "      <td>Lung, lower lobe, left, CT-guided needle biops...</td>\n",
       "      <td>The specimen submitted consists of five tissue...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lung</td>\n",
       "      <td>lower lobe, left</td>\n",
       "      <td>CT-guided needle biopsy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>T1_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EGFR exon 18 mutation: Not detected\\nEGFR exon...</td>\n",
       "      <td>Analysis of EGFR gene mutation\\n(1)Tissue orig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>T1_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Result:\\n1. PD-L1 expression in tumor cells (T...</td>\n",
       "      <td>Pathologic Report for PD-L1 (SP263) Assay (Ven...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lung</td>\n",
       "      <td>lung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>T1_4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ROS1 expression is positive (intensity: modera...</td>\n",
       "      <td>ROS1 expression is positive (intensity: modera...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>T1_5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The ALK expression is negative.\\n\\n</td>\n",
       "      <td>The ALK expression is negative.\\n\\n(1) Descrip...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id part_num  Chat No.                                          path_soap  \\\n",
       "0   1     T1_1  17956402  Lung, lower lobe, left, CT-guided needle biops...   \n",
       "1   2     T1_2       NaN  EGFR exon 18 mutation: Not detected\\nEGFR exon...   \n",
       "2   3     T1_3       NaN  Result:\\n1. PD-L1 expression in tumor cells (T...   \n",
       "3   4     T1_4       NaN  ROS1 expression is positive (intensity: modera...   \n",
       "4   5     T1_5       NaN                The ALK expression is negative.\\n\\n   \n",
       "\n",
       "                                         path_report Cytology_report organ  \\\n",
       "0  The specimen submitted consists of five tissue...             NaN  Lung   \n",
       "1  Analysis of EGFR gene mutation\\n(1)Tissue orig...             NaN  Lung   \n",
       "2  Pathologic Report for PD-L1 (SP263) Assay (Ven...             NaN  lung   \n",
       "3  ROS1 expression is positive (intensity: modera...             NaN   NaN   \n",
       "4  The ALK expression is negative.\\n\\n(1) Descrip...             NaN   NaN   \n",
       "\n",
       "            Bx-site                operation Htype tumor_size  \\\n",
       "0  lower lobe, left  CT-guided needle biopsy   NaN        NaN   \n",
       "1               NaN                      NaN   NaN        NaN   \n",
       "2              lung                      NaN   NaN        NaN   \n",
       "3               NaN                      NaN   NaN        NaN   \n",
       "4               NaN                      NaN   NaN        NaN   \n",
       "\n",
       "  Greatest dimension Tumor_Focality LV_invasion closest_margin version   pT  \\\n",
       "0                NaN            NaN         NaN            NaN     NaN  NaN   \n",
       "1                NaN            NaN         NaN            NaN     NaN  NaN   \n",
       "2                NaN            NaN         NaN            NaN     NaN  NaN   \n",
       "3                NaN            NaN         NaN            NaN     NaN  NaN   \n",
       "4                NaN            NaN         NaN            NaN     NaN  NaN   \n",
       "\n",
       "    pN   pM pStage  \n",
       "0  NaN  NaN    NaN  \n",
       "1  NaN  NaN    NaN  \n",
       "2  NaN  NaN    NaN  \n",
       "3  NaN  NaN    NaN  \n",
       "4  NaN  NaN    NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_path = \"./data/raw/TMUH_pathReport_T1-T4_NER.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a target to report dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_to_report = {\n",
    "    \"organ\": \"path_soap\",\n",
    "    \"Bx-site\": \"Cytology_report\",\n",
    "    \"operation\": \"Cytology_report\",\n",
    "    \"Htype\": \"path_report\",\n",
    "    \"tumor_size\": \"path_report\",\n",
    "    \"Greatest dimension\": \"path_report\",\n",
    "    \"Tumor_Focality\": \"path_report\",\n",
    "    \"LV_invasion\": \"path_report\",\n",
    "    \"closest_margin\": \"path_report\",\n",
    "    \"version\": \"path_report\",\n",
    "    \"pT\": \"path_report\",\n",
    "    \"pN\": \"path_report\",\n",
    "    \"pM\": \"path_report\",\n",
    "    \"pStage\": \"path_report\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51552369bd0a4decabd2b3e18c573b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3042 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "record = []\n",
    "target_list = list(target_to_report.keys())\n",
    "for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "    for target in target_list:\n",
    "        if not pd.isna(row[target]):\n",
    "            report = row[target_to_report[target]]\n",
    "            report = row[\"path_soap\"] if pd.isna(report) else report\n",
    "            assert report is not None, f\"ID {row['id']}: {target} cannot map the report.\"\n",
    "            \n",
    "            # Find target index from report\n",
    "            target_index = []\n",
    "            cur_index = 0\n",
    "            while report.find(row[target], cur_index) != -1:\n",
    "                index = report.find(row[target], cur_index)\n",
    "                target_index.append(str(index))\n",
    "                cur_index = index + 1\n",
    "            \n",
    "            # Not found => find target in path report\n",
    "            if len(target_index) == 0 and not pd.isna(row[\"path_report\"]):\n",
    "                report = row[\"path_report\"]\n",
    "                cur_index = 0\n",
    "                while report.find(row[target], cur_index) != -1:\n",
    "                    index = report.find(row[target], cur_index)\n",
    "                    target_index.append(str(index))\n",
    "                    cur_index = index + 1\n",
    "            \n",
    "            record.append([row[\"id\"], report, target, row[target], len(target_index), \", \".join(target_index)])\n",
    "record = pd.DataFrame(record, columns=[\"id\", \"report\", \"target\", \"target_text\", \"counts\", \"index\"])\n",
    "record.to_excel(\"./outputs/observation/target_counts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"01234\"\n",
    "x = \"0\"\n",
    "t.find(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [3, 5, 6]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [[3,5,6], [1,2,3]]\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3042/3042 [00:00<00:00, 7457.85it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import jsonlines\n",
    "import json\n",
    "\n",
    "data_path = \"./data/raw/TMUH_pathReport_T1-T4_NER.xlsx\"\n",
    "data = pd.read_excel(data_path)\n",
    "# data = data.iloc[:1379,:]\n",
    "\n",
    "def find_entity(text, target, target_text):\n",
    "    entities = []\n",
    "    cur_index = 0\n",
    "    while text.find(target_text, cur_index) != -1:\n",
    "        beging = text.find(target_text, cur_index)\n",
    "        end = beging + len(target_text)\n",
    "        entities.append([beging, end, target])\n",
    "        cur_index = end\n",
    "    return entities\n",
    "\n",
    "report_to_targets = {\n",
    "    \"Cytology_report\": [\"Bx-site\", \"operation\"],\n",
    "    \"path_soap\": [\"organ\"],\n",
    "    \"path_report\": [\n",
    "        \"Htype\",\n",
    "        \"tumor_size\",\n",
    "        \"Greatest dimension\",\n",
    "        \"Tumor_Focality\",\n",
    "        \"LV_invasion\",\n",
    "        \"closest_margin\",\n",
    "        \"version\",\n",
    "        \"pT\",\n",
    "        \"pN\",\n",
    "        \"pM\",\n",
    "        \"pStage\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "outputs = []\n",
    "\n",
    "for i, row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "\n",
    "    no_match_targets = []\n",
    "\n",
    "    for report in [\"Cytology_report\", \"path_soap\", \"path_report\"]:\n",
    "\n",
    "        if not pd.isna(row[report]):\n",
    "\n",
    "            text = row[report]\n",
    "            label = []\n",
    "\n",
    "            if len(no_match_targets) != 0:\n",
    "                match_targets = []\n",
    "                for target in no_match_targets:\n",
    "                    entities = find_entity(text, target, row[target])\n",
    "                    if len(entities) != 0:\n",
    "                        label.extend(entities)\n",
    "                        match_targets.append(target)\n",
    "                no_match_targets = [target for target in no_match_targets if target not in match_targets]\n",
    "\n",
    "            for target in report_to_targets[report]:\n",
    "                if not pd.isna(row[target]):\n",
    "                    entities = find_entity(text, target, row[target])\n",
    "                    if len(entities) == 0:\n",
    "                        no_match_targets.append(target)\n",
    "                    else:\n",
    "                        label.extend(entities)\n",
    "            \n",
    "            label.sort()\n",
    "            outputs.append({\"text\": text, \"label\": label})\n",
    "\n",
    "        else:\n",
    "\n",
    "            for target in report_to_targets[report]:\n",
    "                if not pd.isna(row[target]):\n",
    "                    no_match_targets.append(target)\n",
    "\n",
    "    if len(no_match_targets) != 0:\n",
    "        print(f\"ID {row['id']} has no match targets {no_match_targets}\")\n",
    "\n",
    "with jsonlines.open(\"./data/processed/to-be-annotated-1.jsonl\", \"w\") as writer:\n",
    "    for line in outputs[:(len(outputs)//2)]:\n",
    "        writer.write(line)\n",
    "\n",
    "with jsonlines.open(\"./data/processed/to-be-annotated-2.jsonl\", \"w\") as writer:\n",
    "    for line in outputs[(len(outputs)//2):]:\n",
    "        writer.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f79b45a4a9179c3d9f58334a0802cab802399d9df1f81be44bc30c7400d9157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
