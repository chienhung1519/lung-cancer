{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GatorTron-OG\n",
    "\n",
    "* [GatorTron-OG](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/gatortron_og)\n",
    "* **NOTE**: The output hidden size of GatorTron-OG is 1024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and unzip GatorTron-OG\n",
    "* Model related files are stored in `models/gatortron_og_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/gatortron_og_1\n",
    "!cd models/gatortron_og_1\n",
    "!wget --content-disposition https://api.ngc.nvidia.com/v2/models/nvidia/clara/gatortron_og/versions/1/zip -O gatortron_og_1.zip\n",
    "!unzip gatortron_og_1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify the `model/gatortron_og_1/hparam.yaml` file.\n",
    "* vocab_file change to the **absolute path** of the `model/gatortron_og_1/vocab.txt`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tokenizer:\n",
    "    library: 'megatron'\n",
    "    type: 'BertWordPieceCase'\n",
    "    model: null\n",
    "    vocab_file: /home/chchen/python_work/lung-cancer/models/gatortron_og_1/vocab.txt\n",
    "    merge_file: null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use NEMO to Initialize Model\n",
    "\n",
    "* [Get lm model](https://github.com/NVIDIA/NeMo/blob/1274c10b15374c137a2f64d0e5f8483cd1246440/nemo/collections/nlp/modules/common/lm_utils.py#L52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install APEX\n",
    "* It takes long time.....\n",
    "* **NOTE**: Restart jupyter notebook after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ericharper/apex.git\n",
    "%cd apex\n",
    "!git checkout nm_v1.11.0\n",
    "%pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --global-option=\"--fast_layer_norm\" --global-option=\"--distributed_adam\" --global-option=\"--deprecated_fused_adam\" ./\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NEMO\n",
    "\n",
    "* Python 3.8 or above\n",
    "* Pytorch 1.10.0 or above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install git+https://github.com/NVIDIA/NeMo.git@main#egg=nemo_toolkit[nlp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 11:04:22.859288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-oq2z2rki because the default path (/home/chchen/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n",
      "[NeMo W 2022-10-08 11:04:36 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-10-08 11:04:38 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-10-08 11:04:52 megatron_init:204] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:207] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:208] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:216] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:217] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:227] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:231] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:232] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:246] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:258] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:264] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:265] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:266] All embedding group ranks: [[0]]\n",
      "[NeMo I 2022-10-08 11:04:52 megatron_init:267] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-08 11:04:53 modelPT:217] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-10-08 11:04:53 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-bert-345m-cased, custom vocab file: /home/chchen/python_work/lung-cancer/models/gatortron_og_1/vocab.txt, and merges file: None\n",
      "[NeMo I 2022-10-08 11:04:53 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-large-cased, vocab_file: /home/chchen/python_work/lung-cancer/models/gatortron_og_1/vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2022-10-08 11:05:00 megatron_base_model:186] Padded vocab_size: 50176, original vocab_size: 50101, dummy tokens: 75.\n",
      "[NeMo I 2022-10-08 11:05:04 save_restore_connector:243] Model MegatronBertModel was successfully restored from /home/chchen/python_work/lung-cancer/models/gatortron_og_1/MegatronBERT.nemo.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_bert_model import MegatronBertModel\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return x\n",
    "\n",
    "\n",
    "trainer = Trainer(accelerator='gpu', devices=1)\n",
    "model = MegatronBertModel.restore_from(\n",
    "    restore_path=\"/home/chchen/python_work/lung-cancer/models/gatortron_og_1/MegatronBERT.nemo\", # change to your path\n",
    "    override_config_path=\"/home/chchen/python_work/lung-cancer/models/gatortron_og_1/hparam.yaml\", # change to your path\n",
    "    trainer=trainer\n",
    ")\n",
    "\n",
    "# Remove the headers that are only revelant for pretraining\n",
    "model.model.lm_head = Identity()\n",
    "model.model.binary_head = Identity()\n",
    "model.model.language_model.pooler = Identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1024])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"bert-large-cased\", \n",
    "    vocab_file=\"./models/gatortron_og_1/vocab.txt\", \n",
    "    eos_token=\"[SEP]\", \n",
    "    bos_token=\"[CLS]\"\n",
    ")\n",
    "\n",
    "inputs = [\"Lung cancer\", \"pt report\"]\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "tokenized_inputs.to(\"cuda\")\n",
    "outputs = model.half()(**tokenized_inputs)\n",
    "cls_hidden_states = outputs[0][:, 0, :]\n",
    "print(cls_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0509,  0.0679,  0.3252,  ...,  0.7988, -0.3411, -0.1211],\n",
       "        [ 0.2325, -0.0684, -0.2262,  ...,  0.1630, -0.4092,  0.1421]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From NEMO to HuggingFace\n",
    "\n",
    "* [convert megatron bert checkpoint](https://github.com/huggingface/transformers/blob/main/src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py)\n",
    "* The outputs are different from the NEMO outputs. It seems no resource can resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python convert_megatron_bert_checkpoint.py \\\n",
    "    --path_to_checkpoint ./models/gatortron_og_1/MegatronBERT.pt \\\n",
    "    --config_file ./models/gatortron_og_1/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, MegatronBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./models/gatortron_og_1/\")\n",
    "model = MegatronBertModel.from_pretrained(\"./models/gatortron_og_1/\")\n",
    "\n",
    "inputs = [\"Lung cancer\", \"pt report\"]\n",
    "tokenized_inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
    "outputs = model(**tokenized_inputs)\n",
    "pooler_output = outputs.pooler_output\n",
    "print(pooler_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0815,  0.0063, -0.0151,  ..., -0.1110,  0.1497,  0.1081],\n",
       "        [ 0.1350,  0.0614, -0.1357,  ..., -0.1256,  0.2016,  0.1923]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooler_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f79b45a4a9179c3d9f58334a0802cab802399d9df1f81be44bc30c7400d9157"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
